# By lllyasviel - Mac M æ™¶ç‰‡ç›¸å®¹ç‰ˆæœ¬

import torch

cpu = torch.device('cpu')

# Mac M æ™¶ç‰‡ç›¸å®¹æ€§ä¿®æ”¹
if torch.cuda.is_available():
    gpu = torch.device(f'cuda:{torch.cuda.current_device()}')
    print("ğŸ–¥ï¸ ä½¿ç”¨ CUDA GPU")
elif torch.backends.mps.is_available():
    gpu = torch.device('mps')  # Mac M æ™¶ç‰‡ä½¿ç”¨ MPS
    print("ğŸ ä½¿ç”¨ Mac MPS å¾Œç«¯")
else:
    gpu = torch.device('cpu')
    print("âš ï¸ ä½¿ç”¨ CPUï¼Œæ•ˆèƒ½å¯èƒ½è¼ƒæ…¢")

gpu_complete_modules = []

class DynamicSwapInstaller:
    @staticmethod
    def _install_module(module: torch.nn.Module, **kwargs):
        original_class = module.__class__
        module.__dict__['forge_backup_original_class'] = original_class

        def hacked_get_attr(self, name: str):
            if '_parameters' in self.__dict__:
                _parameters = self.__dict__['_parameters']
                if name in _parameters:
                    p = _parameters[name]
                    if p is None:
                        return None
                    if p.__class__ == torch.nn.Parameter:
                        return torch.nn.Parameter(p.to(**kwargs), requires_grad=p.requires_grad)
                    else:
                        return p.to(**kwargs)
            if '_buffers' in self.__dict__:
                _buffers = self.__dict__['_buffers']
                if name in _buffers:
                    return _buffers[name].to(**kwargs)
            return super(original_class, self).__getattr__(name)

        module.__class__ = type('DynamicSwap_' + original_class.__name__, (original_class,), {
            '__getattr__': hacked_get_attr,
        })

        return

    @staticmethod
    def _uninstall_module(module: torch.nn.Module):
        if 'forge_backup_original_class' in module.__dict__:
            module.__class__ = module.__dict__.pop('forge_backup_original_class')
        return

    @staticmethod
    def install_model(model: torch.nn.Module, **kwargs):
        for m in model.modules():
            DynamicSwapInstaller._install_module(m, **kwargs)
        return

    @staticmethod
    def uninstall_model(model: torch.nn.Module):
        for m in model.modules():
            DynamicSwapInstaller._uninstall_module(m)
        return

def fake_diffusers_current_device(model: torch.nn.Module, target_device: torch.device):
    if hasattr(model, 'scale_shift_table'):
        model.scale_shift_table.data = model.scale_shift_table.data.to(target_device)
        return

    for k, p in model.named_modules():
        if hasattr(p, 'weight'):
            p.to(target_device)
            return

def get_cuda_free_memory_gb(device=None):
    """å–å¾—å¯ç”¨è¨˜æ†¶é«”ï¼ˆMac ç›¸å®¹ç‰ˆæœ¬ï¼‰"""
    if device is None:
        device = gpu

    if device.type == 'cuda' and torch.cuda.is_available():
        try:
            memory_stats = torch.cuda.memory_stats(device)
            bytes_active = memory_stats['active_bytes.all.current']
            bytes_reserved = memory_stats['reserved_bytes.all.current']
            bytes_free_cuda, _ = torch.cuda.mem_get_info(device)
            bytes_inactive_reserved = bytes_reserved - bytes_active
            bytes_total_available = bytes_free_cuda + bytes_inactive_reserved
            return bytes_total_available / (1024 ** 3)
        except:
            return 16.0  # é è¨­å€¼
    elif device.type == 'mps':
        # Mac MPS æ²’æœ‰ç›´æ¥çš„è¨˜æ†¶é«”æŸ¥è©¢ï¼Œè¿”å›é ä¼°å€¼
        try:
            import psutil
            virtual_memory = psutil.virtual_memory()
            # å‡è¨­å¯ç”¨ç³»çµ±è¨˜æ†¶é«”çš„ 70% å¯ä¾› MPS ä½¿ç”¨
            available_gb = (virtual_memory.available * 0.7) / (1024 ** 3)
            return min(available_gb, 32)  # MPS æœ€å¤§ç´„ 32GBï¼ˆçµ±ä¸€è¨˜æ†¶é«”ï¼‰
        except ImportError:
            return 16.0  # é è¨­å€¼ï¼Œé©åˆå¤§å¤šæ•¸ Mac
    else:
        # CPU æ¨¡å¼
        try:
            import psutil
            return psutil.virtual_memory().available / (1024 ** 3)
        except ImportError:
            return 8.0  # é è¨­å€¼

def move_model_to_device_with_memory_preservation(model, target_device, preserved_memory_gb=0):
    print(f'Moving {model.__class__.__name__} to {target_device} with preserved memory: {preserved_memory_gb} GB')

    for m in model.modules():
        current_free_memory = get_cuda_free_memory_gb(target_device)
        if current_free_memory <= preserved_memory_gb:
            _clear_cache(target_device)
            return

        if hasattr(m, 'weight'):
            try:
                m.to(device=target_device)
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•ç§»å‹•æ¨¡çµ„åˆ° {target_device}: {e}")
                break

    try:
        model.to(device=target_device)
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ç§»å‹•æ¨¡å‹åˆ° {target_device}: {e}")
    
    _clear_cache(target_device)
    return

def offload_model_from_device_for_memory_preservation(model, target_device, preserved_memory_gb=0):
    print(f'Offloading {model.__class__.__name__} from {target_device} to preserve memory: {preserved_memory_gb} GB')

    for m in model.modules():
        current_free_memory = get_cuda_free_memory_gb(target_device)
        if current_free_memory >= preserved_memory_gb:
            _clear_cache(target_device)
            return

        if hasattr(m, 'weight'):
            try:
                m.to(device=cpu)
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•å¸è¼‰æ¨¡çµ„: {e}")

    try:
        model.to(device=cpu)
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•å¸è¼‰æ¨¡å‹: {e}")
    
    _clear_cache(target_device)
    return

def unload_complete_models(*args):
    for m in gpu_complete_modules + list(args):
        try:
            m.to(device=cpu)
            print(f'Unloaded {m.__class__.__name__} as complete.')
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•å¸è¼‰æ¨¡å‹ {m.__class__.__name__}: {e}")

    gpu_complete_modules.clear()
    _clear_cache(gpu)
    return

def load_model_as_complete(model, target_device, unload=True):
    if unload:
        unload_complete_models()

    try:
        model.to(device=target_device)
        print(f'Loaded {model.__class__.__name__} to {target_device} as complete.')
        gpu_complete_modules.append(model)
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•è¼‰å…¥æ¨¡å‹åˆ° {target_device}: {e}")
    
    return

def _clear_cache(device):
    """æ¸…ç†è¨˜æ†¶é«”å¿«å–"""
    if device.type == 'cuda' and torch.cuda.is_available():
        torch.cuda.empty_cache()
    elif device.type == 'mps' and torch.backends.mps.is_available():
        try:
            torch.mps.empty_cache()
        except AttributeError:
            # èˆŠç‰ˆ PyTorch å¯èƒ½æ²’æœ‰ mps.empty_cache()
            pass
        except Exception as e:
            print(f"âš ï¸ MPS å¿«å–æ¸…ç†å¤±æ•—: {e}")