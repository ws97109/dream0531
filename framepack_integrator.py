"""
FramePack å®Œæ•´æ•´åˆæ¨¡çµ„ - Mac M æ™¶ç‰‡ç›¸å®¹ç‰ˆæœ¬
ç›´æ¥ä½¿ç”¨ FramePack çš„æ ¸å¿ƒé‚è¼¯ç”Ÿæˆå½±ç‰‡
"""
import os
import sys
import torch
import time
import uuid
import numpy as np
import traceback
from PIL import Image

# è¨­å®š FramePack ç’°å¢ƒ
framepack_path = os.path.join(os.path.dirname(__file__), 'FramePack')
sys.path.insert(0, framepack_path)
os.environ['HF_HOME'] = os.path.join(framepack_path, 'hf_download')

# å°å…¥ FramePack æ ¸å¿ƒçµ„ä»¶
try:
    from diffusers import AutoencoderKLHunyuanVideo
    from transformers import LlamaModel, CLIPTextModel, LlamaTokenizerFast, CLIPTokenizer, SiglipImageProcessor, SiglipVisionModel
    from diffusers_helper.hunyuan import encode_prompt_conds, vae_decode, vae_encode
    from diffusers_helper.utils import save_bcthw_as_mp4, crop_or_pad_yield_mask, soft_append_bcthw, resize_and_center_crop, generate_timestamp
    from diffusers_helper.models.hunyuan_video_packed import HunyuanVideoTransformer3DModelPacked
    from diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan
    from diffusers_helper.memory import cpu, gpu, get_cuda_free_memory_gb, move_model_to_device_with_memory_preservation, offload_model_from_device_for_memory_preservation, fake_diffusers_current_device, unload_complete_models, load_model_as_complete
    from diffusers_helper.clip_vision import hf_clip_vision_encode
    from diffusers_helper.bucket_tools import find_nearest_bucket
    
    FRAMEPACK_AVAILABLE = True
    print("âœ… FramePack æ ¸å¿ƒæ¨¡çµ„è¼‰å…¥æˆåŠŸ")
except ImportError as e:
    FRAMEPACK_AVAILABLE = False
    print(f"âŒ FramePack ä¸å¯ç”¨: {e}")

# å…¨å±€æ¨¡å‹å„²å­˜
_models = {}
_models_loaded = False
_high_vram = False

def is_framepack_available():
    """æª¢æŸ¥ FramePack æ˜¯å¦å¯ç”¨ï¼ˆMac ç›¸å®¹ç‰ˆæœ¬ï¼‰"""
    if not FRAMEPACK_AVAILABLE:
        return False
    
    # Mac M æ™¶ç‰‡æª¢æŸ¥
    if torch.backends.mps.is_available():
        print("ğŸ æª¢æ¸¬åˆ° Mac MPS å¾Œç«¯")
        return True
    elif torch.cuda.is_available():
        try:
            free_mem_gb = get_cuda_free_memory_gb(gpu)
            return free_mem_gb >= 6
        except:
            return False
    else:
        # CPU æ¨¡å¼ä¹Ÿå…è¨±ï¼Œé›–ç„¶æœƒå¾ˆæ…¢
        print("âš ï¸ ä½¿ç”¨ CPU æ¨¡å¼ï¼Œç”Ÿæˆé€Ÿåº¦æœƒè¼ƒæ…¢")
        return True

def load_framepack_models():
    """è¼‰å…¥æ‰€æœ‰ FramePack æ¨¡å‹ï¼ˆMac ç›¸å®¹ç‰ˆæœ¬ï¼‰"""
    global _models, _models_loaded, _high_vram
    
    if _models_loaded:
        return True
    
    if not FRAMEPACK_AVAILABLE:
        print("âŒ FramePack æ¨¡çµ„ä¸å¯ç”¨")
        return False
    
    try:
        print("ğŸ”„ é–‹å§‹è¼‰å…¥ FramePack æ¨¡å‹...")
        
        # æª¢æŸ¥è¨˜æ†¶é«”ï¼ˆé©ç”¨æ–¼ Macï¼‰
        free_mem_gb = get_cuda_free_memory_gb(gpu)
        _high_vram = free_mem_gb > 32  # Mac é€šå¸¸æœ‰è¼ƒå¤šçµ±ä¸€è¨˜æ†¶é«”
        
        print(f'ğŸ’¾ å¯ç”¨è¨˜æ†¶é«”: {free_mem_gb:.1f} GB')
        print(f'ğŸš€ é«˜è¨˜æ†¶é«”æ¨¡å¼: {_high_vram}')
        
        # è¨­å®šé©åˆ Mac çš„è³‡æ–™å‹åˆ¥
        if gpu.type == 'mps':
            # MPS å°æŸäº›æ“ä½œéœ€è¦ float32
            text_dtype = torch.float32
            vae_dtype = torch.float32
            transformer_dtype = torch.float32
        else:
            text_dtype = torch.float16
            vae_dtype = torch.float16
            transformer_dtype = torch.bfloat16
        
        # è¼‰å…¥æ‰€æœ‰æ¨¡å‹
        print("ğŸ“ è¼‰å…¥æ–‡æœ¬ç·¨ç¢¼å™¨...")
        _models['text_encoder'] = LlamaModel.from_pretrained(
            "hunyuanvideo-community/HunyuanVideo", 
            subfolder='text_encoder', 
            torch_dtype=text_dtype
        ).cpu()
        
        _models['text_encoder_2'] = CLIPTextModel.from_pretrained(
            "hunyuanvideo-community/HunyuanVideo", 
            subfolder='text_encoder_2', 
            torch_dtype=text_dtype
        ).cpu()
        
        print("ğŸ”¤ è¼‰å…¥åˆ†è©å™¨...")
        _models['tokenizer'] = LlamaTokenizerFast.from_pretrained(
            "hunyuanvideo-community/HunyuanVideo", 
            subfolder='tokenizer'
        )
        
        _models['tokenizer_2'] = CLIPTokenizer.from_pretrained(
            "hunyuanvideo-community/HunyuanVideo", 
            subfolder='tokenizer_2'
        )
        
        print("ğŸ¨ è¼‰å…¥ VAE...")
        _models['vae'] = AutoencoderKLHunyuanVideo.from_pretrained(
            "hunyuanvideo-community/HunyuanVideo", 
            subfolder='vae', 
            torch_dtype=vae_dtype
        ).cpu()
        
        print("ğŸ–¼ï¸ è¼‰å…¥åœ–åƒç·¨ç¢¼å™¨...")
        _models['feature_extractor'] = SiglipImageProcessor.from_pretrained(
            "lllyasviel/flux_redux_bfl", 
            subfolder='feature_extractor'
        )
        
        _models['image_encoder'] = SiglipVisionModel.from_pretrained(
            "lllyasviel/flux_redux_bfl", 
            subfolder='image_encoder', 
            torch_dtype=text_dtype
        ).cpu()
        
        print("ğŸ¤– è¼‰å…¥ FramePack è®Šæ›å™¨...")
        _models['transformer'] = HunyuanVideoTransformer3DModelPacked.from_pretrained(
            'lllyasviel/FramePack_F1_I2V_HY_20250503', 
            torch_dtype=transformer_dtype
        ).cpu()
        
        # è¨­å®šæ‰€æœ‰æ¨¡å‹
        for model_name in ['vae', 'text_encoder', 'text_encoder_2', 'image_encoder', 'transformer']:
            model = _models[model_name]
            if hasattr(model, 'eval'):
                model.eval()
            if hasattr(model, 'requires_grad_'):
                model.requires_grad_(False)
        
        # è¨˜æ†¶é«”å„ªåŒ–è¨­å®šï¼ˆMac ç‰¹åŒ–ï¼‰
        if not _high_vram or gpu.type == 'mps':
            _models['vae'].enable_slicing()
            _models['vae'].enable_tiling()
        
        # é«˜å“è³ªè¼¸å‡ºè¨­å®š
        _models['transformer'].high_quality_fp32_output_for_inference = True
        
        # è¨­å®šæ•¸æ“šé¡å‹
        _models['transformer'].to(dtype=transformer_dtype)
        _models['vae'].to(dtype=vae_dtype)
        _models['image_encoder'].to(dtype=text_dtype)
        _models['text_encoder'].to(dtype=text_dtype)
        _models['text_encoder_2'].to(dtype=text_dtype)
        
        _models_loaded = True
        print("âœ… æ‰€æœ‰ FramePack æ¨¡å‹è¼‰å…¥å®Œæˆ")
        return True
        
    except Exception as e:
        print(f"âŒ FramePack æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        traceback.print_exc()
        return False

def generate_dream_video(image_path, dream_text, video_length=5, video_quality='standard', static_dir="./static", progress_callback=None):
    """
    ç”Ÿæˆå¤¢å¢ƒå½±ç‰‡ä¸»å‡½æ•¸ï¼ˆMac ç›¸å®¹ç‰ˆæœ¬ï¼‰
    """
    if not FRAMEPACK_AVAILABLE:
        print("âŒ FramePack ä¸å¯ç”¨")
        if progress_callback:
            progress_callback("âŒ FramePack æ¨¡çµ„ä¸å¯ç”¨")
        return None
    
    # è¼‰å…¥æ¨¡å‹
    if not load_framepack_models():
        print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
        if progress_callback:
            progress_callback("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
        return None
    
    try:
        if progress_callback:
            progress_callback("ğŸ–¼ï¸ è™•ç†è¼¸å…¥åœ–åƒ...")
        
        # è¼‰å…¥ä¸¦è™•ç†åœ–åƒ
        input_image = Image.open(image_path)
        if input_image.mode != 'RGB':
            input_image = input_image.convert('RGB')
        input_image_np = np.array(input_image)
        
        # å‰µå»ºå½±ç‰‡æç¤ºè©
        prompt = f"The scene comes alive with gentle movement, {dream_text}, cinematic, smooth motion, detailed"
        n_prompt = ""
        
        # æ ¹æ“šå“è³ªå’Œè¨­å‚™èª¿æ•´åƒæ•¸
        if gpu.type == 'mps':
            # Mac MPS å„ªåŒ–åƒæ•¸
            quality_params = {
                'fast': {'steps': 10, 'gs': 6.0, 'teacache': True, 'crf': 22},
                'standard': {'steps': 15, 'gs': 8.0, 'teacache': True, 'crf': 18},
                'high': {'steps': 20, 'gs': 10.0, 'teacache': False, 'crf': 14}
            }
        else:
            # åŸå§‹åƒæ•¸
            quality_params = {
                'fast': {'steps': 15, 'gs': 8.0, 'teacache': True, 'crf': 20},
                'standard': {'steps': 25, 'gs': 10.0, 'teacache': True, 'crf': 16},
                'high': {'steps': 35, 'gs': 12.0, 'teacache': False, 'crf': 12}
            }
        
        params = quality_params.get(video_quality, quality_params['standard'])
        
        if progress_callback:
            device_info = "Mac MPS" if gpu.type == 'mps' else str(gpu)
            progress_callback(f"ğŸ¬ é–‹å§‹ç”Ÿæˆ {video_length} ç§’å½±ç‰‡ï¼ˆ{video_quality} å“è³ªï¼Œ{device_info}ï¼‰...")
        
        # èª¿ç”¨æ ¸å¿ƒç”Ÿæˆé‚è¼¯
        video_path = _generate_video_core(
            input_image_np, prompt, n_prompt, video_length, 
            params, static_dir, progress_callback
        )
        
        if progress_callback:
            progress_callback("âœ… å½±ç‰‡ç”Ÿæˆå®Œæˆï¼")
        
        return video_path
        
    except Exception as e:
        print(f"âŒ å½±ç‰‡ç”Ÿæˆå¤±æ•—: {e}")
        traceback.print_exc()
        if progress_callback:
            progress_callback(f"âŒ ç”Ÿæˆå¤±æ•—: {str(e)}")
        return None

def _generate_video_core(input_image, prompt, n_prompt, total_second_length, params, static_dir, progress_callback):
    """
    æ ¸å¿ƒå½±ç‰‡ç”Ÿæˆé‚è¼¯ï¼ˆMac æœ€ä½³åŒ–ç‰ˆæœ¬ï¼‰
    """
    # å›ºå®šåƒæ•¸
    seed = 31337
    latent_window_size = 9
    cfg = 1.0
    rs = 0.0
    
    # Mac è¨˜æ†¶é«”ä¿è­·è¨­å®š
    if gpu.type == 'mps':
        gpu_memory_preservation = 4  # Mac MPS è¼ƒå¯¬é¬†çš„è¨˜æ†¶é«”ç®¡ç†
    else:
        gpu_memory_preservation = 6
    
    # å¾åƒæ•¸ä¸­æå–è¨­å®š
    steps = params['steps']
    gs = params['gs']
    use_teacache = params['teacache']
    mp4_crf = params['crf']
    
    # è¨ˆç®—ç¸½æ®µè½æ•¸
    total_latent_sections = (total_second_length * 30) / (latent_window_size * 4)
    total_latent_sections = int(max(round(total_latent_sections), 1))
    
    job_id = generate_timestamp()
    outputs_folder = os.path.join(static_dir, 'videos')
    os.makedirs(outputs_folder, exist_ok=True)
    
    try:
        # æ¸…ç†è¨˜æ†¶é«”
        if not _high_vram:
            unload_complete_models(
                _models['text_encoder'], _models['text_encoder_2'], 
                _models['image_encoder'], _models['vae'], _models['transformer']
            )
        
        # æ–‡æœ¬ç·¨ç¢¼
        if progress_callback:
            progress_callback("ğŸ“ ç·¨ç¢¼æ–‡æœ¬æç¤º...")
        
        if not _high_vram:
            fake_diffusers_current_device(_models['text_encoder'], gpu)
            load_model_as_complete(_models['text_encoder_2'], target_device=gpu)
        
        llama_vec, clip_l_pooler = encode_prompt_conds(
            prompt, _models['text_encoder'], _models['text_encoder_2'], 
            _models['tokenizer'], _models['tokenizer_2']
        )
        
        if cfg == 1:
            llama_vec_n, clip_l_pooler_n = torch.zeros_like(llama_vec), torch.zeros_like(clip_l_pooler)
        else:
            llama_vec_n, clip_l_pooler_n = encode_prompt_conds(
                n_prompt, _models['text_encoder'], _models['text_encoder_2'], 
                _models['tokenizer'], _models['tokenizer_2']
            )
        
        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)
        llama_vec_n, llama_attention_mask_n = crop_or_pad_yield_mask(llama_vec_n, length=512)
        
        # è™•ç†è¼¸å…¥åœ–åƒ
        if progress_callback:
            progress_callback("ğŸ–¼ï¸ è™•ç†åœ–åƒå°ºå¯¸...")
        
        H, W, C = input_image.shape
        height, width = find_nearest_bucket(H, W, resolution=640)
        input_image_np = resize_and_center_crop(input_image, target_width=width, target_height=height)
        
        input_image_pt = torch.from_numpy(input_image_np).float() / 127.5 - 1
        input_image_pt = input_image_pt.permute(2, 0, 1)[None, :, None]
        
        # VAE ç·¨ç¢¼
        if progress_callback:
            progress_callback("ğŸ¨ VAE ç·¨ç¢¼...")
        
        if not _high_vram:
            load_model_as_complete(_models['vae'], target_device=gpu)
        
        start_latent = vae_encode(input_image_pt, _models['vae'])
        
        # CLIP è¦–è¦ºç·¨ç¢¼
        if progress_callback:
            progress_callback("ğŸ‘ï¸ CLIP è¦–è¦ºç·¨ç¢¼...")
        
        if not _high_vram:
            load_model_as_complete(_models['image_encoder'], target_device=gpu)
        
        image_encoder_output = hf_clip_vision_encode(
            input_image_np, _models['feature_extractor'], _models['image_encoder']
        )
        image_encoder_last_hidden_state = image_encoder_output.last_hidden_state
        
        # è¨­å®šæ•¸æ“šé¡å‹ï¼ˆMac MPS ç›¸å®¹ï¼‰
        transformer = _models['transformer']
        target_dtype = transformer.dtype
        
        llama_vec = llama_vec.to(target_dtype)
        llama_vec_n = llama_vec_n.to(target_dtype)
        clip_l_pooler = clip_l_pooler.to(target_dtype)
        clip_l_pooler_n = clip_l_pooler_n.to(target_dtype)
        image_encoder_last_hidden_state = image_encoder_last_hidden_state.to(target_dtype)
        
        # é–‹å§‹æ¡æ¨£
        if progress_callback:
            progress_callback("ğŸš€ é–‹å§‹å½±ç‰‡ç”Ÿæˆæ¡æ¨£...")
        
        rnd = torch.Generator("cpu").manual_seed(seed)
        
        history_latents = torch.zeros(
            size=(1, 16, 16 + 2 + 1, height // 8, width // 8), 
            dtype=torch.float32
        ).cpu()
        
        history_latents = torch.cat([history_latents, start_latent.to(history_latents)], dim=2)
        total_generated_latent_frames = 1
        history_pixels = None
        
        # ç”Ÿæˆå½±ç‰‡æ®µè½
        for section_index in range(total_latent_sections):
            if progress_callback:
                progress = 40 + (section_index / total_latent_sections) * 50
                progress_callback(f"ğŸ¬ ç”Ÿæˆæ®µè½ {section_index + 1}/{total_latent_sections}... ({progress:.0f}%)")
            
            if not _high_vram:
                unload_complete_models()
                move_model_to_device_with_memory_preservation(
                    transformer, target_device=gpu, preserved_memory_gb=gpu_memory_preservation
                )
            
            # åˆå§‹åŒ– TeaCache
            transformer.initialize_teacache(enable_teacache=use_teacache, num_steps=steps)
            
            # è¨­å®šç´¢å¼•
            indices = torch.arange(0, sum([1, 16, 2, 1, latent_window_size])).unsqueeze(0)
            clean_latent_indices_start, clean_latent_4x_indices, clean_latent_2x_indices, clean_latent_1x_indices, latent_indices = indices.split([1, 16, 2, 1, latent_window_size], dim=1)
            clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices], dim=1)
            
            clean_latents_4x, clean_latents_2x, clean_latents_1x = history_latents[:, :, -sum([16, 2, 1]):, :, :].split([16, 2, 1], dim=2)
            clean_latents = torch.cat([start_latent.to(history_latents), clean_latents_1x], dim=2)
            
            # ç”Ÿæˆæ½›åœ¨è®Šé‡
            try:
                generated_latents = sample_hunyuan(
                    transformer=transformer,
                    sampler='unipc',
                    width=width,
                    height=height,
                    frames=latent_window_size * 4 - 3,
                    real_guidance_scale=cfg,
                    distilled_guidance_scale=gs,
                    guidance_rescale=rs,
                    num_inference_steps=steps,
                    generator=rnd,
                    prompt_embeds=llama_vec,
                    prompt_embeds_mask=llama_attention_mask,
                    prompt_poolers=clip_l_pooler,
                    negative_prompt_embeds=llama_vec_n,
                    negative_prompt_embeds_mask=llama_attention_mask_n,
                    negative_prompt_poolers=clip_l_pooler_n,
                    device=gpu,
                    dtype=target_dtype,
                    image_embeddings=image_encoder_last_hidden_state,
                    latent_indices=latent_indices,
                    clean_latents=clean_latents,
                    clean_latent_indices=clean_latent_indices,
                    clean_latents_2x=clean_latents_2x,
                    clean_latent_2x_indices=clean_latent_2x_indices,
                    clean_latents_4x=clean_latents_4x,
                    clean_latent_4x_indices=clean_latent_4x_indices,
                )
            except Exception as e:
                print(f"âš ï¸ æ¡æ¨£éç¨‹å‡ºç¾éŒ¯èª¤: {e}")
                if gpu.type == 'mps':
                    print("ğŸ MPS å¯èƒ½éœ€è¦æ›´å¤šè¨˜æ†¶é«”æˆ–é™ä½å“è³ªè¨­å®š")
                raise
            
            total_generated_latent_frames += int(generated_latents.shape[2])
            history_latents = torch.cat([history_latents, generated_latents.to(history_latents)], dim=2)
            
            # VAE è§£ç¢¼
            if not _high_vram:
                offload_model_from_device_for_memory_preservation(transformer, target_device=gpu, preserved_memory_gb=gpu_memory_preservation)
                load_model_as_complete(_models['vae'], target_device=gpu)
            
            real_history_latents = history_latents[:, :, -total_generated_latent_frames:, :, :]
            
            if history_pixels is None:
                history_pixels = vae_decode(real_history_latents, _models['vae']).cpu()
            else:
                section_latent_frames = latent_window_size * 2
                overlapped_frames = latent_window_size * 4 - 3
                
                current_pixels = vae_decode(real_history_latents[:, :, -section_latent_frames:], _models['vae']).cpu()
                history_pixels = soft_append_bcthw(history_pixels, current_pixels, overlapped_frames)
            
            if not _high_vram:
                unload_complete_models()
        
        # ä¿å­˜å½±ç‰‡
        if progress_callback:
            progress_callback("ğŸ’¾ ä¿å­˜å½±ç‰‡æª”æ¡ˆ...")
        
        output_filename = f"dream_video_{job_id}.mp4"
        output_path = os.path.join(outputs_folder, output_filename)
       
        save_bcthw_as_mp4(history_pixels, output_path, fps=30, crf=mp4_crf)
       
        print(f"âœ… å½±ç‰‡å·²ä¿å­˜: {output_filename}")
        return os.path.join('videos', output_filename)
       
    except Exception as e:
       print(f"âŒ æ ¸å¿ƒç”ŸæˆéŒ¯èª¤: {e}")
       traceback.print_exc()
       
       if not _high_vram:
           unload_complete_models(
               _models['text_encoder'], _models['text_encoder_2'], 
               _models['image_encoder'], _models['vae'], _models['transformer']
           )
       
       return None

# å‘å¾Œç›¸å®¹ä»‹é¢
class FramePackVideoGenerator:
   def __init__(self, static_dir):
       self.static_dir = static_dir
   
   def is_available(self):
       return is_framepack_available()
   
   def generate_video_from_image(self, image_path, prompt, video_length=5, progress_callback=None):
       return generate_dream_video(
           image_path, prompt, video_length, 'standard', 
           self.static_dir, progress_callback
       )

def get_video_generator(static_dir="./static"):
   return FramePackVideoGenerator(static_dir)