"""
FramePack å½±ç‰‡ç”Ÿæˆæ¨¡çµ„
æ•´åˆ FramePack çš„ Image-to-Video åŠŸèƒ½åˆ°å¤¢å¢ƒåˆ†æç³»çµ±ä¸­
"""

import os
import sys
import torch
import time
import uuid
import traceback
import numpy as np
from PIL import Image
from pathlib import Path

# æ·»åŠ  FramePack æ¨¡çµ„è·¯å¾‘
framepack_path = os.path.join(os.path.dirname(__file__), 'FramePack')
if framepack_path not in sys.path:
    sys.path.insert(0, framepack_path)

try:
    from diffusers import AutoencoderKLHunyuanVideo
    from transformers import LlamaModel, CLIPTextModel, LlamaTokenizerFast, CLIPTokenizer, SiglipImageProcessor, SiglipVisionModel
    from diffusers_helper.hunyuan import encode_prompt_conds, vae_decode, vae_encode
    from diffusers_helper.utils import save_bcthw_as_mp4, crop_or_pad_yield_mask, soft_append_bcthw, resize_and_center_crop, generate_timestamp
    from diffusers_helper.models.hunyuan_video_packed import HunyuanVideoTransformer3DModelPacked
    from diffusers_helper.pipelines.k_diffusion_hunyuan import sample_hunyuan
    from diffusers_helper.memory import cpu, gpu, get_cuda_free_memory_gb, move_model_to_device_with_memory_preservation, offload_model_from_device_for_memory_preservation, unload_complete_models, load_model_as_complete
    from diffusers_helper.clip_vision import hf_clip_vision_encode
    from diffusers_helper.bucket_tools import find_nearest_bucket
    FRAMEPACK_AVAILABLE = True
    print("âœ… FramePack æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
except ImportError as e:
    FRAMEPACK_AVAILABLE = False
    print(f"âŒ FramePack æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
    print("è«‹ç¢ºä¿ FramePack è³‡æ–™å¤¾åœ¨æ­£ç¢ºä½ç½®ä¸”ä¾è³´é …ç›®å·²å®‰è£")


class FramePackVideoGenerator:
    """FramePack å½±ç‰‡ç”Ÿæˆå™¨"""
    
    def __init__(self, static_dir="./static"):
        self.static_dir = static_dir
        self.models_loaded = False
        self.device = self._get_device()
        self.models = {}
        
        # å‰µå»ºè¼¸å‡ºç›®éŒ„
        self.videos_dir = os.path.join(static_dir, 'videos')
        os.makedirs(self.videos_dir, exist_ok=True)
        
        print(f"ğŸ¬ FramePack å½±ç‰‡ç”Ÿæˆå™¨åˆå§‹åŒ–ï¼Œä½¿ç”¨è¨­å‚™: {self.device}")
    
    def _get_device(self):
        """ç²å–æœ€ä½³å¯ç”¨è¨­å‚™"""
        if torch.cuda.is_available():
            return torch.device('cuda')
        elif torch.backends.mps.is_available():
            return torch.device('mps')
        else:
            return torch.device('cpu')
    
    def is_available(self):
        """æª¢æŸ¥ FramePack æ˜¯å¦å¯ç”¨"""
        return FRAMEPACK_AVAILABLE and self._check_gpu_requirements()
    
    def _check_gpu_requirements(self):
        """æª¢æŸ¥ GPU è¨˜æ†¶é«”éœ€æ±‚"""
        if self.device.type == 'cuda':
            try:
                free_mem_gb = get_cuda_free_memory_gb(self.device)
                return free_mem_gb >= 6  # FramePack æœ€ä½éœ€æ±‚ 6GB
            except:
                return False
        elif self.device.type == 'mps':
            # MPS æš«æ™‚æ”¯æ´è¼ƒæœ‰é™ï¼Œè¬¹æ…è¿”å›
            return True
        else:
            return False  # CPU æ¨¡å¼é€šå¸¸æ•ˆèƒ½ä¸è¶³
    
    def load_models(self):
        """è¼‰å…¥ FramePack æ¨¡å‹"""
        if not FRAMEPACK_AVAILABLE:
            raise RuntimeError("FramePack æ¨¡çµ„ä¸å¯ç”¨")
        
        if self.models_loaded:
            return True
        
        try:
            print("ğŸ”„ æ­£åœ¨è¼‰å…¥ FramePack æ¨¡å‹...")
            
            # è¨­å®š HuggingFace å¿«å–ç›®éŒ„
            os.environ['HF_HOME'] = os.path.join(os.path.dirname(__file__), 'FramePack', 'hf_download')
            
            # æª¢æŸ¥ GPU è¨˜æ†¶é«”
            free_mem_gb = get_cuda_free_memory_gb(self.device) if self.device.type == 'cuda' else 8
            high_vram = free_mem_gb > 60
            
            print(f"å¯ç”¨ VRAM: {free_mem_gb:.1f} GB, é«˜VRAMæ¨¡å¼: {high_vram}")
            
            # è¼‰å…¥æ–‡æœ¬ç·¨ç¢¼å™¨
            print("ğŸ“ è¼‰å…¥æ–‡æœ¬ç·¨ç¢¼å™¨...")
            self.models['text_encoder'] = LlamaModel.from_pretrained(
                "hunyuanvideo-community/HunyuanVideo", 
                subfolder='text_encoder', 
                torch_dtype=torch.float16
            ).cpu()
            
            self.models['text_encoder_2'] = CLIPTextModel.from_pretrained(
                "hunyuanvideo-community/HunyuanVideo", 
                subfolder='text_encoder_2', 
                torch_dtype=torch.float16
            ).cpu()
            
            # è¼‰å…¥åˆ†è©å™¨
            self.models['tokenizer'] = LlamaTokenizerFast.from_pretrained(
                "hunyuanvideo-community/HunyuanVideo", 
                subfolder='tokenizer'
            )
            
            self.models['tokenizer_2'] = CLIPTokenizer.from_pretrained(
                "hunyuanvideo-community/HunyuanVideo", 
                subfolder='tokenizer_2'
            )
            
            # è¼‰å…¥ VAE
            print("ğŸ¨ è¼‰å…¥ VAE...")
            self.models['vae'] = AutoencoderKLHunyuanVideo.from_pretrained(
                "hunyuanvideo-community/HunyuanVideo", 
                subfolder='vae', 
                torch_dtype=torch.float16
            ).cpu()
            
            # è¼‰å…¥åœ–åƒç·¨ç¢¼å™¨
            print("ğŸ–¼ï¸ è¼‰å…¥åœ–åƒç·¨ç¢¼å™¨...")
            self.models['feature_extractor'] = SiglipImageProcessor.from_pretrained(
                "lllyasviel/flux_redux_bfl", 
                subfolder='feature_extractor'
            )
            
            self.models['image_encoder'] = SiglipVisionModel.from_pretrained(
                "lllyasviel/flux_redux_bfl", 
                subfolder='image_encoder', 
                torch_dtype=torch.float16
            ).cpu()
            
            # è¼‰å…¥ä¸»è¦çš„è®Šæ›å™¨æ¨¡å‹ (ä½¿ç”¨ F1 ç‰ˆæœ¬)
            print("ğŸ¤– è¼‰å…¥ FramePack è®Šæ›å™¨æ¨¡å‹...")
            self.models['transformer'] = HunyuanVideoTransformer3DModelPacked.from_pretrained(
                'lllyasviel/FramePack_F1_I2V_HY_20250503', 
                torch_dtype=torch.bfloat16
            ).cpu()
            
            # è¨­å®šæ‰€æœ‰æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
            for model_name, model in self.models.items():
                if hasattr(model, 'eval'):
                    model.eval()
                if hasattr(model, 'requires_grad_'):
                    model.requires_grad_(False)
            
            # å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
            if not high_vram:
                self.models['vae'].enable_slicing()
                self.models['vae'].enable_tiling()
            
            # å•Ÿç”¨é«˜è³ªé‡è¼¸å‡º
            self.models['transformer'].high_quality_fp32_output_for_inference = True
            
            # è¨­å®šæ•¸æ“šé¡å‹
            self.models['transformer'].to(dtype=torch.bfloat16)
            self.models['vae'].to(dtype=torch.float16)
            self.models['image_encoder'].to(dtype=torch.float16)
            self.models['text_encoder'].to(dtype=torch.float16)
            self.models['text_encoder_2'].to(dtype=torch.float16)
            
            # æ ¹æ“š VRAM æƒ…æ³æ±ºå®šè¼‰å…¥ç­–ç•¥
            if high_vram:
                # é«˜ VRAM æ¨¡å¼ï¼šå…¨éƒ¨è¼‰å…¥åˆ° GPU
                print("ğŸš€ é«˜VRAMæ¨¡å¼ï¼šå°‡æ‰€æœ‰æ¨¡å‹è¼‰å…¥åˆ° GPU")
                for model_name, model in self.models.items():
                    if hasattr(model, 'to'):
                        model.to(self.device)
            else:
                # ä½ VRAM æ¨¡å¼ï¼šä½¿ç”¨å‹•æ…‹å¸è¼‰
                print("ğŸ’¾ ä½VRAMæ¨¡å¼ï¼šå•Ÿç”¨å‹•æ…‹æ¨¡å‹å¸è¼‰")
                # é€™è£¡å¯ä»¥æ ¹æ“šéœ€è¦è¨­å®šå‹•æ…‹å¸è¼‰ç­–ç•¥
            
            self.models_loaded = True
            self.high_vram = high_vram
            
            print("âœ… FramePack æ¨¡å‹è¼‰å…¥å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ FramePack æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            traceback.print_exc()
            return False
    
    def generate_video_from_image(self, image_path, prompt, video_length=5, progress_callback=None):
        """
        å¾åœ–åƒç”Ÿæˆå½±ç‰‡
        
        Args:
            image_path: è¼¸å…¥åœ–åƒè·¯å¾‘
            prompt: å½±ç‰‡ç”Ÿæˆæç¤ºè©
            video_length: å½±ç‰‡é•·åº¦ï¼ˆç§’ï¼‰
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
        
        Returns:
            ç”Ÿæˆçš„å½±ç‰‡æ–‡ä»¶è·¯å¾‘ï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        return self.generate_video_from_image_with_params(
            image_path=image_path,
            prompt=prompt,
            video_length=video_length,
            generation_params={},  # ä½¿ç”¨é è¨­åƒæ•¸
            progress_callback=progress_callback
        )

    def generate_video_from_image_with_params(self, image_path, prompt, video_length=5, generation_params=None, progress_callback=None):
        """
        ä½¿ç”¨æŒ‡å®šåƒæ•¸å¾åœ–åƒç”Ÿæˆå½±ç‰‡
        
        Args:
            image_path: è¼¸å…¥åœ–åƒè·¯å¾‘
            prompt: å½±ç‰‡ç”Ÿæˆæç¤ºè©
            video_length: å½±ç‰‡é•·åº¦ï¼ˆç§’ï¼‰
            generation_params: ç”Ÿæˆåƒæ•¸å­—å…¸
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
        
        Returns:
            ç”Ÿæˆçš„å½±ç‰‡æ–‡ä»¶è·¯å¾‘ï¼Œå¤±æ•—æ™‚è¿”å› None
        """
        if not self.models_loaded:
            if not self.load_models():
                return None
        
        # åˆä½µé è¨­åƒæ•¸
        default_params = {
            'use_teacache': True,
            'num_inference_steps': 25,
            'guidance_scale': 10.0,
            'fps': 30,
            'optimization_level': 'balanced'
        }
        
        if generation_params is None:
            generation_params = {}
        
        params = {**default_params, **generation_params}
        
        try:
            if progress_callback:
                progress_callback("ğŸ–¼ï¸ è¼‰å…¥ä¸¦è™•ç†è¼¸å…¥åœ–åƒ...")
            
            # è¼‰å…¥ä¸¦è™•ç†è¼¸å…¥åœ–åƒ
            input_image = Image.open(image_path)
            if input_image.mode != 'RGB':
                input_image = input_image.convert('RGB')
            
            input_image_np = np.array(input_image)
            
            if progress_callback:
                progress_callback("ğŸ“ ç·¨ç¢¼æ–‡æœ¬æç¤º...")
            
            # ç·¨ç¢¼æ–‡æœ¬æç¤º
            llama_vec, clip_l_pooler = self._encode_text_prompt(prompt)
            
            if progress_callback:
                quality_info = params.get('optimization_level', 'balanced')
                progress_callback(f"ğŸ¬ é–‹å§‹ç”Ÿæˆå½±ç‰‡ï¼ˆ{quality_info}æ¨¡å¼ï¼‰...")
            
            # ä½¿ç”¨åƒæ•¸ç”Ÿæˆå½±ç‰‡
            video_path = self._generate_video_internal_with_params(
                input_image_np, 
                llama_vec, 
                clip_l_pooler, 
                video_length,
                params,
                progress_callback
            )
            
            if progress_callback:
                progress_callback("âœ… å½±ç‰‡ç”Ÿæˆå®Œæˆï¼")
            
            return video_path
            
        except Exception as e:
            print(f"âŒ å½±ç‰‡ç”Ÿæˆå¤±æ•—: {e}")
            traceback.print_exc()
            if progress_callback:
                progress_callback(f"âŒ ç”Ÿæˆå¤±æ•—: {str(e)}")
            return None
    
    def _encode_text_prompt(self, prompt):
        """ç·¨ç¢¼æ–‡æœ¬æç¤º"""
        # è¼‰å…¥æ–‡æœ¬ç·¨ç¢¼å™¨åˆ°è¨­å‚™ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not self.high_vram:
            load_model_as_complete(self.models['text_encoder'], target_device=self.device)
            load_model_as_complete(self.models['text_encoder_2'], target_device=self.device)
        
        # ç·¨ç¢¼æ­£é¢æç¤º
        llama_vec, clip_l_pooler = encode_prompt_conds(
            prompt, 
            self.models['text_encoder'], 
            self.models['text_encoder_2'], 
            self.models['tokenizer'], 
            self.models['tokenizer_2']
        )
        
        # è£å‰ªå’Œå¡«å……åˆ°å›ºå®šé•·åº¦
        llama_vec, llama_attention_mask = crop_or_pad_yield_mask(llama_vec, length=512)
        
        return llama_vec, clip_l_pooler
    
    def _generate_video_internal(self, input_image_np, llama_vec, clip_l_pooler, video_length, progress_callback):
        """å…§éƒ¨å½±ç‰‡ç”Ÿæˆé‚è¼¯ï¼ˆä¿ç•™åŸå§‹æ–¹æ³•ä»¥å‘å¾Œå…¼å®¹ï¼‰"""
        return self._generate_video_internal_with_params(
            input_image_np, 
            llama_vec, 
            clip_l_pooler, 
            video_length,
            {},  # ä½¿ç”¨é è¨­åƒæ•¸
            progress_callback
        )

    def _generate_video_internal_with_params(self, input_image_np, llama_vec, clip_l_pooler, video_length, params, progress_callback):
        """
        ä½¿ç”¨æŒ‡å®šåƒæ•¸çš„å…§éƒ¨å½±ç‰‡ç”Ÿæˆé‚è¼¯
        """
        
        # èª¿æ•´åœ–åƒå¤§å°åˆ°åˆé©çš„è§£æåº¦
        H, W, C = input_image_np.shape
        height, width = find_nearest_bucket(H, W, resolution=640)
        input_image_np = resize_and_center_crop(input_image_np, target_width=width, target_height=height)
        
        # è½‰æ›ç‚º PyTorch å¼µé‡
        input_image_pt = torch.from_numpy(input_image_np).float() / 127.5 - 1
        input_image_pt = input_image_pt.permute(2, 0, 1)[None, :, None]
        
        if progress_callback:
            progress_callback("ğŸ¨ VAE ç·¨ç¢¼åœ–åƒ...")
        
        # VAE ç·¨ç¢¼
        if not self.high_vram:
            load_model_as_complete(self.models['vae'], target_device=self.device)
        
        start_latent = vae_encode(input_image_pt, self.models['vae'])
        
        if progress_callback:
            progress_callback("ğŸ‘ï¸ CLIP è¦–è¦ºç·¨ç¢¼...")
        
        # CLIP è¦–è¦ºç·¨ç¢¼
        if not self.high_vram:
            load_model_as_complete(self.models['image_encoder'], target_device=self.device)
        
        image_encoder_output = hf_clip_vision_encode(
            input_image_np, 
            self.models['feature_extractor'], 
            self.models['image_encoder']
        )
        image_encoder_last_hidden_state = image_encoder_output.last_hidden_state
        
        # è¨­å®šæ•¸æ“šé¡å‹
        transformer = self.models['transformer']
        llama_vec = llama_vec.to(transformer.dtype)
        clip_l_pooler = clip_l_pooler.to(transformer.dtype)
        image_encoder_last_hidden_state = image_encoder_last_hidden_state.to(transformer.dtype)
        
        if progress_callback:
            progress_callback("ğŸš€ é–‹å§‹æ“´æ•£æ¡æ¨£...")
        
        # è¨ˆç®—å½±ç‰‡åƒæ•¸
        latent_window_size = 9  # é è¨­çª—å£å¤§å°
        total_latent_sections = (video_length * 30) / (latent_window_size * 4)
        total_latent_sections = int(max(round(total_latent_sections), 1))
        
        # è¼‰å…¥è®Šæ›å™¨æ¨¡å‹åˆ°è¨­å‚™
        if not self.high_vram:
            unload_complete_models()
            move_model_to_device_with_memory_preservation(
                transformer, 
                target_device=self.device, 
                preserved_memory_gb=6
            )
        
        # æ ¹æ“šåƒæ•¸è¨­å®šåˆå§‹åŒ– TeaCache
        use_teacache = params.get('use_teacache', True)
        num_steps = params.get('num_inference_steps', 25)
        transformer.initialize_teacache(enable_teacache=use_teacache, num_steps=num_steps)
        
        if progress_callback and use_teacache:
            progress_callback(f"âš¡ å·²å•Ÿç”¨ TeaCache åŠ é€Ÿï¼ˆ{num_steps} æ­¥é©Ÿï¼‰")
        elif progress_callback:
            progress_callback(f"ğŸ”§ ä½¿ç”¨æ¨™æº–æ¨¡å¼ï¼ˆ{num_steps} æ­¥é©Ÿï¼‰")
        
        # ç”Ÿæˆåƒæ•¸
        rnd = torch.Generator("cpu").manual_seed(42)  # å›ºå®šç¨®å­ä»¥ç¢ºä¿ä¸€è‡´æ€§
        
        # å‰µå»ºæ­·å²æ½›åœ¨ç©ºé–“
        history_latents = torch.zeros(
            size=(1, 16, 16 + 2 + 1, height // 8, width // 8), 
            dtype=torch.float32
        ).cpu()
        
        history_latents = torch.cat([history_latents, start_latent.to(history_latents)], dim=2)
        total_generated_latent_frames = 1
        history_pixels = None
        
        # å¾åƒæ•¸ç²å–æŒ‡å°å°ºåº¦
        guidance_scale = params.get('guidance_scale', 10.0)
        
        # ç”Ÿæˆå½±ç‰‡æ®µè½
        for section_index in range(total_latent_sections):
            if progress_callback:
                progress = 50 + (section_index / total_latent_sections) * 40
                progress_callback(f"ğŸ¬ ç”Ÿæˆå½±ç‰‡æ®µè½ {section_index + 1}/{total_latent_sections}... ({progress:.0f}%)")
            
            # è¨­å®šç´¢å¼•
            indices = torch.arange(0, sum([1, 16, 2, 1, latent_window_size])).unsqueeze(0)
            clean_latent_indices_start, clean_latent_4x_indices, clean_latent_2x_indices, clean_latent_1x_indices, latent_indices = indices.split([1, 16, 2, 1, latent_window_size], dim=1)
            clean_latent_indices = torch.cat([clean_latent_indices_start, clean_latent_1x_indices], dim=1)
            
            # æº–å‚™æ¸…æ½”æ½›åœ¨è®Šæ•¸
            clean_latents_4x, clean_latents_2x, clean_latents_1x = history_latents[:, :, -sum([16, 2, 1]):, :, :].split([16, 2, 1], dim=2)
            clean_latents = torch.cat([start_latent.to(history_latents), clean_latents_1x], dim=2)
            
            # æ¡æ¨£ç”Ÿæˆï¼ˆä½¿ç”¨åƒæ•¸åŒ–è¨­å®šï¼‰
            generated_latents = sample_hunyuan(
                transformer=transformer,
                sampler='unipc',
                width=width,
                height=height,
                frames=latent_window_size * 4 - 3,
                real_guidance_scale=1.0,
                distilled_guidance_scale=guidance_scale,  # ä½¿ç”¨åƒæ•¸åŒ–æŒ‡å°å°ºåº¦
                guidance_rescale=0.0,
                num_inference_steps=num_steps,  # ä½¿ç”¨åƒæ•¸åŒ–æ­¥æ•¸
                generator=rnd,
                prompt_embeds=llama_vec,
                prompt_embeds_mask=torch.ones((1, llama_vec.shape[1]), dtype=torch.bool, device=llama_vec.device),
                prompt_poolers=clip_l_pooler,
                negative_prompt_embeds=torch.zeros_like(llama_vec),
                negative_prompt_embeds_mask=torch.ones((1, llama_vec.shape[1]), dtype=torch.bool, device=llama_vec.device),
                negative_prompt_poolers=torch.zeros_like(clip_l_pooler),
                device=self.device,
                dtype=torch.bfloat16,
                image_embeddings=image_encoder_last_hidden_state,
                latent_indices=latent_indices,
                clean_latents=clean_latents,
                clean_latent_indices=clean_latent_indices,
                clean_latents_2x=clean_latents_2x,
                clean_latent_2x_indices=clean_latent_2x_indices,
                clean_latents_4x=clean_latents_4x,
                clean_latent_4x_indices=clean_latent_4x_indices,
            )
            
            # æ›´æ–°æ­·å²
            total_generated_latent_frames += int(generated_latents.shape[2])
            history_latents = torch.cat([history_latents, generated_latents.to(history_latents)], dim=2)
            
            # VAE è§£ç¢¼
            if not self.high_vram:
                offload_model_from_device_for_memory_preservation(transformer, target_device=self.device, preserved_memory_gb=8)
                load_model_as_complete(self.models['vae'], target_device=self.device)
            
            real_history_latents = history_latents[:, :, -total_generated_latent_frames:, :, :]
            
            if history_pixels is None:
                history_pixels = vae_decode(real_history_latents, self.models['vae']).cpu()
            else:
                section_latent_frames = latent_window_size * 2
                overlapped_frames = latent_window_size * 4 - 3
                
                current_pixels = vae_decode(real_history_latents[:, :, -section_latent_frames:], self.models['vae']).cpu()
                history_pixels = soft_append_bcthw(history_pixels, current_pixels, overlapped_frames)
            
            if not self.high_vram:
                unload_complete_models()
        
        if progress_callback:
            progress_callback("ğŸ’¾ ä¿å­˜å½±ç‰‡æ–‡ä»¶...")
        
        # ä¿å­˜å½±ç‰‡ï¼ˆä½¿ç”¨åƒæ•¸åŒ–çš„ FPS å’Œå“è³ªï¼‰
        timestamp = int(time.time())
        random_id = str(uuid.uuid4())[:8]
        output_filename = f"dream_video_{timestamp}_{random_id}.mp4"
        output_path = os.path.join(self.videos_dir, output_filename)
        
        fps = params.get('fps', 30)
        # æ ¹æ“šå„ªåŒ–ç­‰ç´šèª¿æ•´ CRFï¼ˆå£“ç¸®ç‡ï¼‰
        optimization_level = params.get('optimization_level', 'balanced')
        if optimization_level == 'high' or optimization_level == 'quality':
            crf = 12  # æ›´é«˜å“è³ªï¼Œæ›´å¤§æ–‡ä»¶
        elif optimization_level == 'fast' or optimization_level == 'aggressive':
            crf = 20  # è¼ƒä½å“è³ªï¼Œè¼ƒå°æ–‡ä»¶
        else:
            crf = 16  # å¹³è¡¡
        
        save_bcthw_as_mp4(history_pixels, output_path, fps=fps, crf=crf)
        
        # è¿”å›ç›¸å°è·¯å¾‘
        return os.path.join('videos', output_filename)
    
    def cleanup_models(self):
        """æ¸…ç†æ¨¡å‹ä»¥é‡‹æ”¾è¨˜æ†¶é«”"""
        if self.models_loaded:
            print("ğŸ§¹ æ¸…ç† FramePack æ¨¡å‹...")
            if not self.high_vram:
                unload_complete_models()
            
            # æ¸…ç† GPU è¨˜æ†¶é«”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            elif torch.backends.mps.is_available():
                try:
                    torch.mps.empty_cache()
                except:
                    pass
    
    def __del__(self):
        """ææ§‹å‡½æ•¸ï¼Œç¢ºä¿æ¸…ç†è³‡æº"""
        try:
            self.cleanup_models()
        except:
            pass


# å…¨å±€å¯¦ä¾‹
_video_generator = None

def get_video_generator(static_dir="./static"):
    """ç²å–å½±ç‰‡ç”Ÿæˆå™¨å¯¦ä¾‹ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰"""
    global _video_generator
    if _video_generator is None:
        _video_generator = FramePackVideoGenerator(static_dir)
    return _video_generator

def generate_dream_video(image_path, dream_text, video_length=5, video_quality='standard', static_dir="./static", progress_callback=None):
    """
    ç”Ÿæˆå¤¢å¢ƒå½±ç‰‡çš„ä¾¿æ·å‡½æ•¸
    
    Args:
        image_path: è¼¸å…¥åœ–åƒè·¯å¾‘
        dream_text: å¤¢å¢ƒæè¿°æ–‡æœ¬
        video_length: å½±ç‰‡é•·åº¦ï¼ˆç§’ï¼‰
        video_quality: å½±ç‰‡å“è³ª ('fast', 'standard', 'high')
        static_dir: éœæ…‹æ–‡ä»¶ç›®éŒ„
        progress_callback: é€²åº¦å›èª¿å‡½æ•¸
    
    Returns:
        å½±ç‰‡æ–‡ä»¶çš„ç›¸å°è·¯å¾‘ï¼Œå¤±æ•—æ™‚è¿”å› None
    """
    generator = get_video_generator(static_dir)
    
    if not generator.is_available():
        print("âŒ FramePack ä¸å¯ç”¨æˆ– GPU è¨˜æ†¶é«”ä¸è¶³")
        if progress_callback:
            progress_callback("âŒ å½±ç‰‡ç”Ÿæˆä¸å¯ç”¨")
        return None
    
    # å‰µå»ºé©åˆå½±ç‰‡ç”Ÿæˆçš„æç¤ºè©
    video_prompt = f"The scene comes alive with gentle movement, {dream_text}, cinematic, smooth motion, detailed"
    
    # æ ¹æ“šå“è³ªè¨­å®šç”Ÿæˆåƒæ•¸
    generation_params = {
        'fast': {
            'use_teacache': True,
            'num_inference_steps': 15,
            'guidance_scale': 8.0,
            'optimization_level': 'aggressive'
        },
        'standard': {
            'use_teacache': True,
            'num_inference_steps': 25,
            'guidance_scale': 10.0,
            'optimization_level': 'balanced'
        },
        'high': {
            'use_teacache': False,
            'num_inference_steps': 35,
            'guidance_scale': 12.0,
            'optimization_level': 'quality'
        }
    }.get(video_quality, {})
    
    return generator.generate_video_from_image_with_params(
        image_path=image_path,
        prompt=video_prompt,
        video_length=video_length,
        generation_params=generation_params,
        progress_callback=progress_callback
    )


if __name__ == "__main__":
    # æ¸¬è©¦è…³æœ¬
    print("ğŸ§ª æ¸¬è©¦ FramePack å½±ç‰‡ç”Ÿæˆå™¨...")
    
    generator = get_video_generator()
    
    if generator.is_available():
        print("âœ… FramePack å¯ç”¨")
        
        # æ¸¬è©¦æ¨¡å‹è¼‰å…¥
        if generator.load_models():
            print("âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸ")
        else:
            print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—")
    else:
        print("âŒ FramePack ä¸å¯ç”¨")
    
    print("æ¸¬è©¦å®Œæˆ")