import os
import time
import uuid
import json
import random
from flask import Flask, request, jsonify, render_template, url_for
import requests
from PIL import Image
import numpy as np
import torch
from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler
import gc

# å°å…¥ FramePack å½±ç‰‡ç”Ÿæˆå™¨
from framepack_video_generator import generate_dream_video, get_video_generator

class DreamAnalyzer:
    def __init__(self):
        self.app_root = os.path.dirname(os.path.abspath(__file__))
        self.static_dir = os.path.join(self.app_root, 'static')
        self.app = Flask(__name__, 
                        template_folder=os.path.join(self.app_root, 'templates'),
                        static_folder=self.static_dir)
        
        # é…ç½®
        self.OLLAMA_API = "http://localhost:11434/api/generate"
        self.OLLAMA_MODEL = "qwen2.5:14b"
        
        # æ¨¡å‹ç‹€æ…‹
        self.image_pipe = None
        self.models_loaded = False
        self.current_device = None
        self.torch_dtype = None
        
        # FramePack å½±ç‰‡ç”Ÿæˆå™¨
        self.video_generator = get_video_generator(self.static_dir)
        
        # å¯ç”¨æ¨¡å‹
        self.models = {
            "stable-diffusion-v1-5": "runwayml/stable-diffusion-v1-5",
        }
        
        # é˜²é‡è¤‡æäº¤
        self.processing_requests = set()  # æ­£åœ¨è™•ç†çš„è«‹æ±‚ID
        self.request_lock = False  # å…¨å±€é–
        
        self._setup_routes()
        self._create_directories()

    def _create_directories(self):
        """å‰µå»ºå¿…è¦çš„ç›®éŒ„"""
        directories = [
            os.path.join(self.static_dir, 'images'),
            os.path.join(self.static_dir, 'videos'),  # æ–°å¢å½±ç‰‡ç›®éŒ„
            os.path.join(self.static_dir, 'shares')
        ]
        for directory in directories:
            os.makedirs(directory, exist_ok=True)

    def _setup_routes(self):
        """è¨­å®šè·¯ç”±"""
        self.app.route('/')(self.index)
        self.app.route('/api/status')(self.api_status)
        self.app.route('/api/analyze', methods=['POST'])(self.analyze)
        self.app.route('/api/share', methods=['POST'])(self.share_result)
        self.app.route('/share/<share_id>')(self.view_shared)

    def _initialize_device(self):
        """åˆå§‹åŒ–è¨­å‚™è¨­å®š"""
        if torch.cuda.is_available():
            self.current_device = "cuda"
            self.torch_dtype = torch.float16
        elif torch.backends.mps.is_available():
            self.current_device = "mps"
            self.torch_dtype = torch.float32
        else:
            self.current_device = "cpu"
            self.torch_dtype = torch.float32

    def _load_image_model(self):
        """è¼‰å…¥åœ–åƒç”Ÿæˆæ¨¡å‹"""
        if self.models_loaded:
            return True
        
        try:
            self._initialize_device()
            model_id = "runwayml/stable-diffusion-v1-5"
            
            self.image_pipe = StableDiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=self.torch_dtype,
                use_safetensors=True,
                safety_checker=None,
                requires_safety_checker=False
            ).to(self.current_device)
            
            self.image_pipe.scheduler = UniPCMultistepScheduler.from_config(
                self.image_pipe.scheduler.config
            )
            
            # å„ªåŒ–è¨­å®š
            if self.current_device == "cuda":
                self.image_pipe.enable_model_cpu_offload()
                self.image_pipe.enable_attention_slicing()
                self.image_pipe.enable_vae_slicing()
            elif self.current_device == "mps":
                self.image_pipe.enable_attention_slicing(1)
            else:
                self.image_pipe.enable_attention_slicing()
            
            self.models_loaded = True
            print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼Œä½¿ç”¨è¨­å‚™: {self.current_device}")
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
            return False

    def _check_ollama_status(self):
        """æª¢æŸ¥ Ollama æœå‹™ç‹€æ…‹"""
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            return response.status_code == 200
        except:
            return False

    def _check_framepack_status(self):
        """æª¢æŸ¥ FramePack å½±ç‰‡ç”Ÿæˆå¯ç”¨æ€§"""
        try:
            return self.video_generator.is_available()
        except:
            return False

    def _call_ollama(self, system_prompt, user_prompt, temperature=0.7):
        """èª¿ç”¨ Ollama API"""
        try:
            data = {
                "model": self.OLLAMA_MODEL,
                "prompt": user_prompt,
                "system": system_prompt,
                "stream": False,
                "options": {
                    "temperature": temperature,
                    "top_p": 0.9,
                    "num_predict": 500
                }
            }
            
            response = requests.post(self.OLLAMA_API, json=data, timeout=120)
            
            if response.status_code == 200:
                result = response.json()
                return result.get("response", "").strip()
            return ""
        except Exception as e:
            print(f"âŒ Ollama èª¿ç”¨å¤±æ•—: {e}")
            return ""

    def _generate_story(self, dream_text):
        """ç”Ÿæˆå¤¢å¢ƒæ•…äº‹"""
        system_prompt = """ä½ æ˜¯å¤¢å¢ƒæ•…äº‹å‰µä½œå°ˆå®¶ï¼Œè¦æ±‚ï¼š
1. ç›´æ¥é–‹å§‹æ•…äº‹ï¼Œç„¡å•å€™èª
2. èåˆå¤¢å¢ƒå…ƒç´ 
3. ä½¿ç”¨ç¬¬ä¸€äººç¨±
4. 150-200å­—å®Œæ•´æ•…äº‹
5. ç¹é«”ä¸­æ–‡"""

        user_prompt = f"åŸºæ–¼å¤¢å¢ƒç‰‡æ®µå‰µä½œæ•…äº‹ï¼šã€Œ{dream_text}ã€"
        
        story = self._call_ollama(system_prompt, user_prompt)
        return self._clean_story_content(story) if story else "ç„¡æ³•ç”Ÿæˆå¤¢å¢ƒæ•…äº‹"

    def _clean_story_content(self, story):
        """æ¸…ç†æ•…äº‹å…§å®¹"""
        unwanted_phrases = [
            "å¥½çš„ï¼Œæ ¹æ“šæ‚¨çš„å»ºè­°", "æ ¹æ“šæ‚¨çš„è¦æ±‚", "ä»¥ä¸‹æ˜¯æ•…äº‹", "æ•…äº‹å¦‚ä¸‹",
            "###", "**", "æ•…äº‹åç¨±ï¼š", "å¤¢å¢ƒæ•…äº‹ï¼š", "å®Œæ•´æ•…äº‹ï¼š"
        ]
        
        cleaned_story = story.strip()
        
        for phrase in unwanted_phrases:
            if cleaned_story.startswith(phrase):
                cleaned_story = cleaned_story[len(phrase):].strip()
            if phrase in cleaned_story:
                parts = cleaned_story.split(phrase)
                if len(parts) > 1:
                    cleaned_story = parts[-1].strip()
        
        # ç§»é™¤å¼•è™Ÿ
        if cleaned_story.startswith('"') and cleaned_story.endswith('"'):
            cleaned_story = cleaned_story[1:-1].strip()
        if cleaned_story.startswith('ã€Œ') and cleaned_story.endswith('ã€'):
            cleaned_story = cleaned_story[1:-1].strip()
        
        return cleaned_story.replace('*', '').replace('#', '').strip()

    def _generate_image_prompt(self, dream_text):
        """å°‡å¤¢å¢ƒè½‰æ›ç‚ºåœ–åƒç”Ÿæˆæç¤ºè©"""
        system_prompt = """You are a Stable Diffusion prompt expert. Convert user input into English image generation prompts. Requirements:
        1. PRESERVE and translate ALL original elements from the input
        2. If input is a story/emotion, extract the MOST VISUAL and EMOTIONAL scene
        3. ENHANCE with additional quality details, don't replace original content
        4. Use English keywords only
        5. The main characters are mostly Asian
        6. Focus on the most dramatic or emotional moment in the story
        7. Include facial expressions and emotions from the original text

        Example:
        Input: "ä»–å¸¸å¤¢è¦‹å‰ä»»çµå©šäº†ï¼Œæ¯æ¬¡éƒ½ç¬‘è‘—ç¥ç¦ï¼Œç„¶å¾Œå“­è‘—é†’ä¾†"
        Better output: "Asian man dreaming, wedding scene, forced smile, tears, emotional pain, bittersweet expression, dream-like atmosphere, cinematic lighting, detailed"

        Always capture the EMOTIONAL CORE and most visual elements."""
        user_prompt = f"Story: {dream_text}\nConvert to image prompt:"
        
        raw_prompt = self._call_ollama(system_prompt, user_prompt, temperature=0.5)
        
        if not raw_prompt:
            return None
        
        # æ¸…ç†ä¸¦è™•ç†æç¤ºè©
        clean_prompt = raw_prompt.strip()
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«äººç‰©å…ƒç´ 
        if ('æˆ‘' in dream_text or 'è‡ªå·±' in dream_text) and \
           not any(word in clean_prompt.lower() for word in ['person', 'human', 'figure']):
            clean_prompt = f" I {clean_prompt}"
        
        final_prompt = f"{clean_prompt}, vibrant colors, detailed, cinematic"
        
        print(f"âœ¨ ç”Ÿæˆåœ–åƒæç¤ºè©: {final_prompt}")
        return final_prompt

    def _generate_image(self, dream_text):
        """ç”Ÿæˆåœ–åƒ"""
        if not self._load_image_model() or self.image_pipe is None:
            print("âŒ åœ–åƒæ¨¡å‹æœªè¼‰å…¥")
            return None
        
        try:
            # ç”Ÿæˆæç¤ºè©
            image_prompt = self._generate_image_prompt(dream_text)
            if not image_prompt:
                print("âŒ ç„¡æ³•ç”Ÿæˆåœ–åƒæç¤ºè©")
                return None
            
            # è¨­å®šè² é¢æç¤ºè©
            person_keywords = ['æˆ‘', 'äºº', 'è‡ªå·±', 'å¤¢è¦‹æˆ‘']
            if any(keyword in dream_text for keyword in person_keywords):
                negative_prompt = "ugly, blurry, distorted, deformed, low quality, bad anatomy, multiple heads, extra limbs"
            else:
                negative_prompt = "human, person, ugly, blurry, distorted, deformed, low quality, bad anatomy"
            
            # ç”Ÿæˆåƒæ•¸
            seed = random.randint(0, 2**32 - 1)
            generator = torch.Generator(self.current_device).manual_seed(seed)
            
            generation_params = {
                "prompt": image_prompt,
                "negative_prompt": negative_prompt,
                "height": 512,
                "width": 512,
                "num_inference_steps": 20 if self.current_device != "cpu" else 15,
                "guidance_scale": 7.5,
                "generator": generator
            }
            
            # æ¸…ç†è¨˜æ†¶é«”
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            
            # ç”Ÿæˆåœ–åƒ
            print("ğŸ¨ é–‹å§‹ç”Ÿæˆåœ–åƒ...")
            with torch.no_grad():
                result = self.image_pipe(**generation_params)
                
                if not result.images or len(result.images) == 0:
                    print("âŒ åœ–åƒç”Ÿæˆå¤±æ•—")
                    return None
                
                generated_image = result.images[0]
            
            # è™•ç†åœ–åƒ
            if generated_image.mode != 'RGB':
                generated_image = generated_image.convert('RGB')
            
            # èª¿æ•´äº®åº¦
            img_array = np.array(generated_image)
            avg_brightness = np.mean(img_array)
            
            if avg_brightness < 30:
                img_array = np.clip(img_array * 1.3 + 20, 0, 255).astype(np.uint8)
                generated_image = Image.fromarray(img_array)
            
            # ä¿å­˜åœ–åƒ
            timestamp = int(time.time())
            random_id = str(uuid.uuid4())[:8]
            output_filename = f"dream_{timestamp}_{random_id}.png"
            
            output_dir = os.path.join(self.static_dir, 'images')
            output_path = os.path.join(output_dir, output_filename)
            
            generated_image.save(output_path, format='PNG', quality=95)
            print(f"âœ… åœ–åƒå·²ä¿å­˜: {output_filename}")
            
            return os.path.join('images', output_filename)
            
        except Exception as e:
            print(f"âŒ åœ–åƒç”ŸæˆéŒ¯èª¤: {e}")
            return None

    def _generate_video(self, image_path, dream_text, progress_callback=None):
        """ç”Ÿæˆå¤¢å¢ƒå½±ç‰‡ï¼ˆåŸå§‹æ–¹æ³•ï¼Œä¿æŒå‘å¾Œå…¼å®¹ï¼‰"""
        return self._generate_video_with_settings(
            image_path, dream_text, 5, 'standard', progress_callback
        )[0]  # åªè¿”å›è·¯å¾‘

    def _generate_video_with_settings(self, image_path, dream_text, video_length, video_quality, progress_callback=None):
        """
        æ ¹æ“šè¨­å®šç”Ÿæˆå¤¢å¢ƒå½±ç‰‡
        
        Args:
            image_path: åœ–åƒè·¯å¾‘
            dream_text: å¤¢å¢ƒæè¿°
            video_length: å½±ç‰‡é•·åº¦ï¼ˆç§’ï¼‰
            video_quality: å½±ç‰‡å“è³ª ('fast', 'standard', 'high')
            progress_callback: é€²åº¦å›èª¿å‡½æ•¸
        
        Returns:
            tuple: (å½±ç‰‡è·¯å¾‘, ç”Ÿæˆè³‡è¨Š)
        """
        if not self._check_framepack_status():
            print("âŒ FramePack å½±ç‰‡ç”Ÿæˆä¸å¯ç”¨")
            if progress_callback:
                progress_callback("âŒ å½±ç‰‡ç”ŸæˆåŠŸèƒ½ä¸å¯ç”¨")
            return None, None
        
        try:
            # å®Œæ•´çš„åœ–åƒè·¯å¾‘
            full_image_path = os.path.join(self.static_dir, image_path)
            
            if not os.path.exists(full_image_path):
                print(f"âŒ åœ–åƒæ–‡ä»¶ä¸å­˜åœ¨: {full_image_path}")
                return None, None
            
            print(f"ğŸ¬ é–‹å§‹ç”Ÿæˆå½±ç‰‡ï¼Œè¼¸å…¥åœ–åƒ: {full_image_path}")
            print(f"ğŸ“Š å½±ç‰‡è¨­å®š: {video_length}ç§’, {video_quality}å“è³ª")
            
            # æ ¹æ“šå“è³ªè¨­å®šèª¿æ•´ç”Ÿæˆåƒæ•¸
            generation_params = self._get_video_generation_params(video_quality)
            
            # ç”Ÿæˆå½±ç‰‡
            start_time = time.time()
            video_path = self._generate_video_with_params(
                image_path=full_image_path,
                dream_text=dream_text,
                video_length=video_length,
                generation_params=generation_params,
                progress_callback=progress_callback
            )
            generation_time = time.time() - start_time
            
            if video_path:
                # æº–å‚™ç”Ÿæˆè³‡è¨Š
                video_info = {
                    'length': video_length,
                    'quality': video_quality,
                    'generationTime': round(generation_time, 1),
                    'parameters': generation_params
                }
                
                print(f"âœ… å½±ç‰‡ç”ŸæˆæˆåŠŸ: {video_path}")
                print(f"â±ï¸ ç”Ÿæˆæ™‚é–“: {generation_time:.1f} ç§’")
                return video_path, video_info
            else:
                print("âŒ å½±ç‰‡ç”Ÿæˆå¤±æ•—")
                return None, None
                
        except Exception as e:
            print(f"âŒ å½±ç‰‡ç”ŸæˆéŒ¯èª¤: {e}")
            if progress_callback:
                progress_callback(f"âŒ å½±ç‰‡ç”Ÿæˆå¤±æ•—: {str(e)}")
            return None, None

    def _get_video_generation_params(self, video_quality):
        """
        æ ¹æ“šå“è³ªè¨­å®šç²å–ç”Ÿæˆåƒæ•¸
        
        Args:
            video_quality: 'fast', 'standard', 'high'
        
        Returns:
            dict: ç”Ÿæˆåƒæ•¸
        """
        base_params = {
            'use_teacache': True,
            'num_inference_steps': 25,
            'guidance_scale': 10.0,
            'fps': 30
        }
        
        if video_quality == 'fast':
            # å¿«é€Ÿæ¨¡å¼ï¼šé™ä½æ­¥æ•¸ï¼Œå•Ÿç”¨æ‰€æœ‰åŠ é€ŸåŠŸèƒ½
            return {
                **base_params,
                'use_teacache': True,
                'num_inference_steps': 15,
                'guidance_scale': 8.0,
                'optimization_level': 'aggressive'
            }
        elif video_quality == 'high':
            # é«˜å“è³ªæ¨¡å¼ï¼šå¢åŠ æ­¥æ•¸ï¼Œé—œé–‰ä¸€äº›åŠ é€ŸåŠŸèƒ½
            return {
                **base_params,
                'use_teacache': False,  # é—œé–‰ TeaCache ä»¥ç²å¾—æ›´å¥½å“è³ª
                'num_inference_steps': 35,
                'guidance_scale': 12.0,
                'optimization_level': 'quality'
            }
        else:
            # æ¨™æº–æ¨¡å¼ï¼šå¹³è¡¡è¨­å®š
            return {
                **base_params,
                'optimization_level': 'balanced'
            }

    def _generate_video_with_params(self, image_path, dream_text, video_length, generation_params, progress_callback=None):
        """
        ä½¿ç”¨æŒ‡å®šåƒæ•¸ç”Ÿæˆå½±ç‰‡
        """
        try:
            # å°å…¥ FramePack ç”Ÿæˆå‡½æ•¸
            from framepack_video_generator import generate_dream_video
            
            # å¾ç”Ÿæˆåƒæ•¸ä¸­æå–å“è³ªè¨­å®š
            optimization_level = generation_params.get('optimization_level', 'balanced')
            
            # å°‡å„ªåŒ–ç­‰ç´šæ˜ å°„åˆ°å“è³ªè¨­å®š
            quality_mapping = {
                'aggressive': 'fast',
                'balanced': 'standard', 
                'quality': 'high'
            }
            video_quality = quality_mapping.get(optimization_level, 'standard')
            
            # ç”Ÿæˆå½±ç‰‡
            video_path = generate_dream_video(
                image_path=image_path,
                dream_text=dream_text,
                video_length=video_length,
                video_quality=video_quality,
                static_dir=self.static_dir,
                progress_callback=progress_callback
            )
            
            return video_path
            
        except Exception as e:
            print(f"âŒ åƒæ•¸åŒ–å½±ç‰‡ç”ŸæˆéŒ¯èª¤: {e}")
            # é™ç´šåˆ°åŸå§‹æ–¹æ³•
            return self._generate_video(image_path.replace(self.static_dir + '/', ''), dream_text, progress_callback)

    def _analyze_psychology(self, dream_text):
        """å¿ƒç†åˆ†æ"""
        system_prompt = """ä½ æ˜¯å¤¢å¢ƒå¿ƒç†åˆ†æå°ˆå®¶ï¼Œåˆ†æå¤¢å¢ƒçš„è±¡å¾µæ„ç¾©å’Œå¿ƒç†ç‹€æ…‹ï¼Œ
æä¾›150-200å­—çš„åˆ†æï¼Œä½¿ç”¨æº«å’Œæ”¯æŒæ€§èªèª¿ï¼Œç¹é«”ä¸­æ–‡å›ç­”ã€‚"""
        
        user_prompt = f"å¤¢å¢ƒæè¿°: {dream_text}\nè«‹æä¾›å¿ƒç†åˆ†æï¼š"
        
        analysis = self._call_ollama(system_prompt, user_prompt)
        return analysis if analysis else "æš«æ™‚ç„¡æ³•é€²è¡Œå¿ƒç†åˆ†æã€‚"

    def _save_dream_result(self, data):
        """ä¿å­˜å¤¢å¢ƒåˆ†æçµæœ"""
        try:
            share_id = str(uuid.uuid4())
            share_dir = os.path.join(self.static_dir, 'shares')
            
            share_data = {
                'id': share_id,
                'timestamp': int(time.time()),
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
                'finalStory': data.get('finalStory', ''),
                'imagePath': data.get('imagePath', ''),
                'videoPath': data.get('videoPath', ''),  # æ–°å¢å½±ç‰‡è·¯å¾‘
                'psychologyAnalysis': data.get('psychologyAnalysis', '')
            }
            
            share_file = os.path.join(share_dir, f"{share_id}.json")
            with open(share_file, 'w', encoding='utf-8') as f:
                json.dump(share_data, f, ensure_ascii=False, indent=2)
            
            return share_id
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ†äº«çµæœå¤±æ•—: {e}")
            return None

    # è·¯ç”±è™•ç†å‡½æ•¸
    def index(self):
        return render_template('index.html')

    def api_status(self):
        ollama_status = self._check_ollama_status()
        local_models_status = self.models_loaded or self._load_image_model()
        framepack_status = self._check_framepack_status()
        
        return jsonify({
            'ollama': ollama_status,
            'local_models': local_models_status,
            'framepack_video': framepack_status,  # æ–°å¢ FramePack ç‹€æ…‹
            'device': self.current_device,
            'available_models': list(self.models.keys()),
            'timestamp': int(time.time())
        })

    def analyze(self):
        data = request.json
        dream_text = data.get('dream', '')
        selected_model = data.get('model', 'stable-diffusion-v1-5')
        generate_video = data.get('generateVideo', False)  # æ˜¯å¦ç”Ÿæˆå½±ç‰‡
        video_length = data.get('videoLength', 5)  # å½±ç‰‡é•·åº¦ï¼ˆç§’ï¼‰
        video_quality = data.get('videoQuality', 'standard')  # å½±ç‰‡å“è³ª
        
        # è¼¸å…¥é©—è­‰
        if not dream_text or len(dream_text.strip()) < 10:
            return jsonify({'error': 'è«‹è¼¸å…¥è‡³å°‘10å€‹å­—çš„å¤¢å¢ƒæè¿°'}), 400
        
        if len(dream_text.strip()) > 2000:
            return jsonify({'error': 'å¤¢å¢ƒæè¿°éé•·ï¼Œè«‹æ§åˆ¶åœ¨2000å­—ä»¥å…§'}), 400
        
        # å½±ç‰‡åƒæ•¸é©—è­‰
        if generate_video:
            if not isinstance(video_length, int) or video_length < 3 or video_length > 15:
                return jsonify({'error': 'å½±ç‰‡é•·åº¦å¿…é ˆåœ¨ 3-15 ç§’ä¹‹é–“'}), 400
            
            if video_quality not in ['fast', 'standard', 'high']:
                return jsonify({'error': 'ç„¡æ•ˆçš„å½±ç‰‡å“è³ªè¨­å®š'}), 400
        
        # é˜²é‡è¤‡æäº¤æª¢æŸ¥
        request_id = f"{dream_text[:50]}_{int(time.time())}"
        
        if self.request_lock:
            print("âš ï¸  æœ‰å…¶ä»–è«‹æ±‚æ­£åœ¨è™•ç†ä¸­ï¼Œè«‹ç¨å€™...")
            return jsonify({'error': 'ç³»çµ±æ­£åœ¨è™•ç†å…¶ä»–è«‹æ±‚ï¼Œè«‹ç¨å€™å†è©¦'}), 429
        
        if request_id in self.processing_requests:
            print("âš ï¸  ç›¸åŒè«‹æ±‚å·²åœ¨è™•ç†ä¸­...")
            return jsonify({'error': 'ç›¸åŒçš„è«‹æ±‚æ­£åœ¨è™•ç†ä¸­'}), 429
        
        # è¨­å®šè™•ç†ç‹€æ…‹
        self.request_lock = True
        self.processing_requests.add(request_id)
        
        try:
            print(f"ğŸŒ™ é–‹å§‹åˆ†æå¤¢å¢ƒ [ID: {request_id[:20]}...]: {dream_text[:50]}...")
            if generate_video:
                print(f"ğŸ¬ æ­¤æ¬¡åˆ†æå°‡åŒ…å« {video_length} ç§’ {video_quality} å“è³ªå½±ç‰‡ç”Ÿæˆ")
            
            # æª¢æŸ¥æœå‹™ç‹€æ…‹
            ollama_status = self._check_ollama_status()
            if not ollama_status:
                return jsonify({'error': 'Ollamaæœå‹™ä¸å¯ç”¨'}), 503
            
            # ç”Ÿæˆæ•…äº‹
            print("ğŸ“– ç”Ÿæˆå¤¢å¢ƒæ•…äº‹...")
            final_story = self._generate_story(dream_text)
            
            # ç”Ÿæˆåœ–åƒ
            print("ğŸ¨ ç”Ÿæˆå¤¢å¢ƒåœ–åƒ...")
            local_models_status = self._load_image_model()
            image_path = None
            if local_models_status:
                image_path = self._generate_image(dream_text)
            
            # ç”Ÿæˆå½±ç‰‡ï¼ˆå¦‚æœè«‹æ±‚ï¼‰
            video_path = None
            video_generation_info = None
            if generate_video and image_path:
                print(f"ğŸ¬ ç”Ÿæˆ {video_length} ç§’å¤¢å¢ƒå½±ç‰‡ï¼ˆ{video_quality} å“è³ªï¼‰...")
                
                def video_progress_callback(message):
                    print(f"ğŸ“¹ å½±ç‰‡ç”Ÿæˆé€²åº¦: {message}")
                
                video_path, video_generation_info = self._generate_video_with_settings(
                    image_path, dream_text, video_length, video_quality, video_progress_callback
                )
            
            # å¿ƒç†åˆ†æ
            print("ğŸ§  é€²è¡Œå¿ƒç†åˆ†æ...")
            psychology_analysis = self._analyze_psychology(dream_text)
            
            response = {
                'finalStory': final_story,
                'imagePath': '/static/' + image_path if image_path else None,
                'videoPath': '/static/' + video_path if video_path else None,
                'psychologyAnalysis': psychology_analysis,
                'apiStatus': {
                    'ollama': ollama_status,
                    'local_models': local_models_status,
                    'framepack_video': self._check_framepack_status(),
                    'device': self.current_device,
                    'current_model': selected_model
                },
                'processingInfo': {
                    'timestamp': int(time.time()),
                    'inputLength': len(dream_text),
                    'storyLength': len(final_story) if final_story else 0,
                    'videoGenerated': video_path is not None,
                    'videoInfo': video_generation_info,  # åŒ…å«å½±ç‰‡è©³ç´°è³‡è¨Š
                    'requestId': request_id[:20]
                }
            }
            
            print(f"âœ… å¤¢å¢ƒåˆ†æå®Œæˆ [ID: {request_id[:20]}...]")
            if video_path:
                print(f"ğŸ¬ å½±ç‰‡å·²ç”Ÿæˆ: {video_path} ({video_length}ç§’, {video_quality}å“è³ª)")
            
            return jsonify(response)
            
        except Exception as e:
            print(f"âŒ åˆ†æéŒ¯èª¤ [ID: {request_id[:20]}...]: {e}")
            return jsonify({'error': 'è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤'}), 500
        
        finally:
            # æ¸…ç†è™•ç†ç‹€æ…‹
            self.request_lock = False
            self.processing_requests.discard(request_id)
            print(f"ğŸ”“ é‡‹æ”¾è«‹æ±‚é– [ID: {request_id[:20]}...]")

    def share_result(self):
        data = request.json
        
        if not data or 'finalStory' not in data:
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦çš„å¤¢å¢ƒåˆ†ææ•¸æ“š'}), 400
        
        try:
            share_id = self._save_dream_result(data)
            
            if not share_id:
                return jsonify({'error': 'å‰µå»ºåˆ†äº«å¤±æ•—'}), 500
            
            share_url = url_for('view_shared', share_id=share_id, _external=True)
            
            return jsonify({
                'shareId': share_id, 
                'shareUrl': share_url,
                'timestamp': int(time.time())
            })
            
        except Exception as e:
            print(f"âŒ åˆ†äº«è™•ç†éŒ¯èª¤: {e}")
            return jsonify({'error': 'è™•ç†åˆ†äº«è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤'}), 500

    def view_shared(self, share_id):
        try:
            share_file = os.path.join(self.static_dir, 'shares', f"{share_id}.json")
            
            if not os.path.exists(share_file):
                return jsonify({'error': 'æ‰¾ä¸åˆ°è©²åˆ†äº«å…§å®¹'}), 404
            
            with open(share_file, 'r', encoding='utf-8') as f:
                share_data = json.load(f)
            
            return render_template('shared.html', data=share_data)
            
        except Exception as e:
            print(f"âŒ è¼‰å…¥åˆ†äº«å…§å®¹éŒ¯èª¤: {e}")
            return jsonify({'error': 'è¼‰å…¥åˆ†äº«å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤'}), 500

    def run(self, debug=False, host='0.0.0.0', port=5002):
        """å•Ÿå‹•æ‡‰ç”¨"""
        print("ğŸš€ å•Ÿå‹•å¤¢å¢ƒåˆ†ææ‡‰ç”¨...")
        print(f"ğŸ“ FramePack å½±ç‰‡ç”Ÿæˆ: {'âœ… å¯ç”¨' if self._check_framepack_status() else 'âŒ ä¸å¯ç”¨'}")
        self.app.run(debug=debug, host=host, port=port, threaded=True)


# ä¸»ç¨‹å¼å…¥å£
if __name__ == '__main__':
    dream_analyzer = DreamAnalyzer()
    dream_analyzer.run(debug=False)