import os
import time
import subprocess
import tempfile
import uuid
import json
import logging
import sys
from flask import Flask, request, jsonify, render_template, url_for
import requests
import base64
from PIL import Image, ImageDraw, ImageFont
from io import BytesIO
import numpy as np

# æ–°å¢çš„å°å…¥
import torch
from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline
import cv2

# è¨­å®šæ—¥èªŒ
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

app_root = os.path.dirname(os.path.abspath(__file__))
static_dir = os.path.join(app_root, 'static')

app = Flask(__name__, 
            template_folder=os.path.join(app_root, 'templates'),
            static_folder=static_dir)

# ==================== é…ç½®è¨­å®š ====================
# Ollama APIï¼ˆåƒ…ç”¨æ–¼æ–‡æœ¬ç”Ÿæˆï¼‰
OLLAMA_API = "http://localhost:11434/api/generate"

# å…¨å±€è®Šæ•¸
image_pipe = None
video_pipe = None
models_loaded = False

# ==================== æœ¬åœ°æ¨¡å‹åˆå§‹åŒ– ====================

def initialize_local_models():
    """åˆå§‹åŒ–æœ¬åœ° Diffusers æ¨¡å‹"""
    global image_pipe, video_pipe, models_loaded
    
    if models_loaded:
        return True
    
    try:
        logger.info("åˆå§‹åŒ–æœ¬åœ° Diffusers æ¨¡å‹...")
        
        # æª¢æ¸¬è¨­å‚™
        if torch.backends.mps.is_available():
            device = "mps"
            torch_dtype = torch.float16
        elif torch.cuda.is_available():
            device = "cuda"
            torch_dtype = torch.float16
        else:
            device = "cpu"
            torch_dtype = torch.float32
        
        logger.info(f"ä½¿ç”¨è¨­å‚™: {device}")
        
        # æ–‡å­—è½‰åœ–åƒæ¨¡å‹ - ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹
        logger.info("è¼‰å…¥åœ–åƒç”Ÿæˆæ¨¡å‹...")
        image_pipe = StableDiffusionPipeline.from_pretrained(
            "runwayml/stable-diffusion-v1-5",
            torch_dtype=torch_dtype,
            use_safetensors=True,
            variant="fp16" if torch_dtype == torch.float16 else None,
            safety_checker=None,          # ç¦ç”¨å®‰å…¨æª¢æŸ¥å™¨
            requires_safety_checker=False  # ä¸éœ€è¦å®‰å…¨æª¢æŸ¥å™¨
        )
        image_pipe = image_pipe.to(device)
        
        # å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–
        if device != "mps":  # MPS æš«ä¸æ”¯æ´æŸäº›å„ªåŒ–
            image_pipe.enable_model_cpu_offload()
        image_pipe.enable_attention_slicing()
        
        # åœ–åƒè½‰è¦–é »æ¨¡å‹ - ä½¿ç”¨è¼•é‡ç‰ˆæœ¬
        logger.info("è¼‰å…¥è¦–é »ç”Ÿæˆæ¨¡å‹...")
        video_pipe = StableVideoDiffusionPipeline.from_pretrained(
            "stabilityai/stable-video-diffusion-img2vid-xt",
            torch_dtype=torch_dtype,
            variant="fp16" if torch_dtype == torch.float16 else None
        )
        video_pipe = video_pipe.to(device)
        
        # è¦–é »æ¨¡å‹è¨˜æ†¶é«”å„ªåŒ–
        if device != "mps":
            video_pipe.enable_model_cpu_offload()
        
        models_loaded = True
        logger.info("âœ… æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        return True
        
    except Exception as e:
        logger.error(f"æ¨¡å‹åˆå§‹åŒ–å¤±æ•—: {str(e)}")
        return False

# ==================== æ•…äº‹è½‰åœ–åƒæç¤ºè© ====================

def story_to_image_prompt(story_text):
    """å°‡æ•…äº‹å…§å®¹è½‰æ›ç‚ºé©åˆåœ–åƒç”Ÿæˆçš„æç¤ºè©"""
    try:
        system_prompt = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„AIç¹ªç•«æç¤ºè©å°ˆå®¶ã€‚è«‹å°‡ç”¨æˆ¶æä¾›çš„ä¸­æ–‡æ•…äº‹å…§å®¹è½‰æ›ç‚ºé©åˆStable Diffusionåœ–åƒç”Ÿæˆçš„è‹±æ–‡æç¤ºè©ã€‚

è¦æ±‚ï¼š
1. æå–æ•…äº‹ä¸­çš„æ ¸å¿ƒè¦–è¦ºå…ƒç´ 
2. è½‰æ›ç‚ºç°¡æ½”çš„è‹±æ–‡é—œéµè©
3. æŒ‰é‡è¦æ€§æ’åºï¼Œæœ€é‡è¦çš„æ”¾å‰é¢
4. åŒ…å«ç•«é¢é¢¨æ ¼ã€è‰²å½©ã€æƒ…å¢ƒç­‰æè¿°
5. é¿å…éæ–¼è¤‡é›œçš„å¥å­ï¼Œä½¿ç”¨é€—è™Ÿåˆ†éš”çš„é—œéµè©
6. é•·åº¦æ§åˆ¶åœ¨50-80å€‹è‹±æ–‡å–®è©å…§

æ ¼å¼ç¯„ä¾‹ï¼š
ä¸»è¦å°è±¡, å‹•ä½œ/ç‹€æ…‹, ç’°å¢ƒ/èƒŒæ™¯, è‰²å½©é¢¨æ ¼, ç•«é¢è³ªé‡è©

è«‹åªè¿”å›è‹±æ–‡æç¤ºè©ï¼Œä¸è¦è§£é‡‹ã€‚"""

        user_prompt = f"""
        æ•…äº‹å…§å®¹ï¼š{story_text}
        
        è«‹è½‰æ›ç‚ºStable Diffusionåœ–åƒç”Ÿæˆæç¤ºè©ï¼š
        """
        
        # ä½¿ç”¨ Ollama é€²è¡Œè½‰æ›
        converted_prompt = ollama_generate(system_prompt, user_prompt, "qwen2.5:14b")
        
        if not converted_prompt:
            # å¦‚æœ Ollama å¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„é—œéµè©æå–
            converted_prompt = extract_visual_keywords(story_text)
        
        # æ·»åŠ è³ªé‡å¢å¼·è©
        enhanced_prompt = f"{converted_prompt}, masterpiece, best quality, highly detailed, cinematic lighting, beautiful composition"
        
        logger.info(f"æ•…äº‹è½‰æ›ç‚ºæç¤ºè©: {enhanced_prompt[:100]}...")
        return enhanced_prompt
        
    except Exception as e:
        logger.error(f"æ•…äº‹è½‰æç¤ºè©å¤±æ•—: {str(e)}")
        return extract_visual_keywords(story_text)

def extract_visual_keywords(text):
    """ç°¡å–®çš„è¦–è¦ºé—œéµè©æå–ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰"""
    # ä¸­è‹±å°ç…§çš„é—œéµè©æ˜ å°„
    keyword_map = {
        # äººç‰©
        'å¥³å­©': 'girl', 'ç”·å­©': 'boy', 'å¥³äºº': 'woman', 'ç”·äºº': 'man',
        'å…¬ä¸»': 'princess', 'ç‹å­': 'prince', 'å¤©ä½¿': 'angel',
        
        # å‹•ä½œ
        'é£›è¡Œ': 'flying', 'æ¼‚æµ®': 'floating', 'è·³èˆ': 'dancing', 'å¥”è·‘': 'running',
        'æ¸¸æ³³': 'swimming', 'è¡Œèµ°': 'walking', 'åè‘—': 'sitting',
        
        # ç’°å¢ƒ
        'æµ·æ´‹': 'ocean', 'å¤§æµ·': 'sea', 'å¤©ç©º': 'sky', 'é›²æœµ': 'clouds',
        'æ£®æ—': 'forest', 'å±±': 'mountain', 'åŸå ¡': 'castle', 'èŠ±åœ’': 'garden',
        'æˆ¿é–“': 'room', 'æ©‹': 'bridge', 'å³¶å¶¼': 'island',
        
        # è‰²å½©
        'è—è‰²': 'blue', 'ç´…è‰²': 'red', 'ç¶ è‰²': 'green', 'é»ƒè‰²': 'yellow',
        'ç´«è‰²': 'purple', 'ç™½è‰²': 'white', 'é»‘è‰²': 'black', 'é‡‘è‰²': 'golden',
        'å½©è™¹': 'rainbow', 'é–ƒå…‰': 'glowing', 'æ˜äº®': 'bright',
        
        # æƒ…å¢ƒ
        'å¤¢å¢ƒ': 'dreamlike', 'å¹»æƒ³': 'fantasy', 'é­”æ³•': 'magical', 'ç¥ç§˜': 'mysterious',
        'ç¾éº—': 'beautiful', 'å„ªé›…': 'elegant', 'æµªæ¼«': 'romantic',
        'æ—¥è½': 'sunset', 'å¤œæ™š': 'night', 'æ˜Ÿç©º': 'starry sky'
    }
    
    extracted_keywords = []
    text_lower = text.lower()
    
    for chinese, english in keyword_map.items():
        if chinese in text:
            extracted_keywords.append(english)
    
    if not extracted_keywords:
        extracted_keywords = ['dreamlike scene', 'fantasy', 'beautiful']
    
    return ', '.join(extracted_keywords[:8])  # æœ€å¤š8å€‹é—œéµè©

# ==================== å¿«é€Ÿåœ–åƒç”Ÿæˆ ====================

def generate_image_fast_local(story_text):
    """å¿«é€Ÿæœ¬åœ°åœ–åƒç”Ÿæˆ"""
    try:
        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        if not initialize_local_models():
            logger.error("æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–å¤±æ•—")
            return create_default_image(f"error_{int(time.time())}.png", story_text)
        
        # å°‡æ•…äº‹è½‰æ›ç‚ºåœ–åƒæç¤ºè©
        image_prompt = story_to_image_prompt(story_text)
        
        logger.info(f"é–‹å§‹ç”Ÿæˆåœ–åƒï¼Œæç¤ºè©: {image_prompt[:50]}...")
        
        # ç”Ÿæˆåƒæ•¸ï¼ˆé‡å°é€Ÿåº¦å„ªåŒ–ï¼‰
        generation_params = {
            "prompt": image_prompt,
            "negative_prompt": "blurry, low quality, distorted, ugly, bad anatomy, deformed, watermark, signature",
            "num_inference_steps": 20,  # æ¸›å°‘æ­¥æ•¸æå‡é€Ÿåº¦
            "guidance_scale": 7.5,
            "width": 512,  # æ¨™æº–å°ºå¯¸
            "height": 512,
            "generator": torch.Generator().manual_seed(42)  # å›ºå®šç¨®å­ç¢ºä¿ä¸€è‡´æ€§
        }
        
        # ç”Ÿæˆåœ–åƒ
        start_time = time.time()
        result = image_pipe(**generation_params)
        generation_time = time.time() - start_time
        
        logger.info(f"åœ–åƒç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {generation_time:.2f}ç§’")
        
        # ä¿å­˜åœ–åƒ
        timestamp = int(time.time())
        random_id = str(uuid.uuid4())[:8]
        output_filename = f"dream_{timestamp}_{random_id}.png"
        
        output_dir = os.path.join(static_dir, 'images')
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        
        result.images[0].save(output_path)
        
        logger.info(f"âœ… åœ–åƒä¿å­˜æˆåŠŸ: {output_filename}")
        return os.path.join('images', output_filename)
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿåœ–åƒç”Ÿæˆå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return create_default_image(f"error_{int(time.time())}.png", story_text)

# ==================== å¿«é€Ÿè¦–é »ç”Ÿæˆ ====================

def generate_video_fast_local(image_path, story_text):
    """å¿«é€Ÿæœ¬åœ°è¦–é »ç”Ÿæˆ"""
    try:
        # ç¢ºä¿æ¨¡å‹å·²è¼‰å…¥
        if not initialize_local_models():
            logger.error("æœ¬åœ°æ¨¡å‹åˆå§‹åŒ–å¤±æ•—")
            return None
        
        # è¼‰å…¥åœ–åƒ
        full_image_path = os.path.join(static_dir, image_path)
        input_image = Image.open(full_image_path)
        
        # èª¿æ•´åœ–åƒå°ºå¯¸ï¼ˆSVD éœ€è¦ç‰¹å®šå°ºå¯¸ï¼‰
        input_image = input_image.resize((1024, 576), Image.Resampling.LANCZOS)
        
        logger.info("é–‹å§‹ç”Ÿæˆè¦–é »...")
        
        # ç”Ÿæˆåƒæ•¸ï¼ˆé‡å°é€Ÿåº¦å„ªåŒ–ï¼‰
        video_params = {
            "image": input_image,
            "decode_chunk_size": 8,  # æ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
            "generator": torch.Generator().manual_seed(42),
            "motion_bucket_id": 127,  # ä¸­ç­‰é‹å‹•å¼·åº¦
            "noise_aug_strength": 0.1,  # è¼ƒä½çš„å™ªè²å¢å¼·
            "num_frames": 14,  # è¼ƒå°‘å¹€æ•¸ï¼ˆç´„0.6ç§’@25fpsï¼‰
        }
        
        start_time = time.time()
        frames = video_pipe(**video_params).frames[0]
        generation_time = time.time() - start_time
        
        logger.info(f"è¦–é »å¹€ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {generation_time:.2f}ç§’")
        
        # ä¿å­˜è¦–é »
        timestamp = int(time.time())
        random_id = str(uuid.uuid4())[:8]
        output_filename = f"dream_video_{timestamp}_{random_id}.mp4"
        
        output_dir = os.path.join(static_dir, 'videos')
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, output_filename)
        
        # ä½¿ç”¨ OpenCV ä¿å­˜è¦–é »ï¼ˆæ›´å¿«ï¼‰
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_path, fourcc, 8.0, (1024, 576))  # 8 FPS
        
        for frame in frames:
            frame_array = np.array(frame)
            frame_bgr = cv2.cvtColor(frame_array, cv2.COLOR_RGB2BGR)
            out.write(frame_bgr)
        
        out.release()
        
        logger.info(f"âœ… è¦–é »ä¿å­˜æˆåŠŸ: {output_filename}")
        return os.path.join('videos', output_filename)
        
    except Exception as e:
        logger.error(f"å¿«é€Ÿè¦–é »ç”Ÿæˆå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ==================== å…¶ä»–è¼”åŠ©å‡½æ•¸ ====================

def check_local_services():
    """æª¢æŸ¥æœ¬åœ°æœå‹™ç‹€æ…‹"""
    try:
        # æª¢æŸ¥ Ollama
        ollama_response = requests.get("http://localhost:11434/api/tags", timeout=5)
        ollama_status = ollama_response.status_code == 200
        logger.info(f"Ollama API ç‹€æ…‹: {'æ­£å¸¸' if ollama_status else 'ç•°å¸¸'}")
        
        # æª¢æŸ¥æœ¬åœ°æ¨¡å‹ç‹€æ…‹
        local_models_status = models_loaded or initialize_local_models()
        
        return ollama_status, True, local_models_status  # fooocus_status è¨­ç‚º Trueï¼ˆä¸å†ä½¿ç”¨ï¼‰
    except Exception as e:
        logger.error(f"æœå‹™æª¢æŸ¥å¤±æ•—: {str(e)}")
        return False, False, False

def dream_weaver(prompt):
    """ä½¿ç”¨Ollamaçš„qwenæ¨¡å‹è™•ç†å¤¢å¢ƒæ•…äº‹ç”Ÿæˆ"""
    try:
        system_planner = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ã€‚ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡æ•…äº‹å¤§ç¶±è¦åŠƒå°ˆå®¶ï¼Œç‰¹åˆ¥æ“…é•·åˆ†æå¤¢å¢ƒå…ƒç´ ä¸¦å‰µå»ºé€£è²«çš„æ•…äº‹æ¶æ§‹ã€‚
        ç•¶ç”¨æˆ¶æä¾›é›¶æ•£çš„å¤¢å¢ƒç‰‡æ®µæ™‚ï¼Œä½ çš„ä»»å‹™æ˜¯åˆ†ææ¯å€‹å…ƒç´ çš„è±¡å¾µæ„ç¾©ï¼Œç”¢ç”Ÿå¤šç¨®å¯èƒ½çš„æ•…äº‹æ¶æ§‹ï¼Œ
        ä¸¦æ ¹æ“šå…ƒç´ ä¹‹é–“çš„æ½›åœ¨è¯ç¹«é¸æ“‡æœ€ä½³æ¶æ§‹ã€‚è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ã€‚"""

        system_reflector = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ã€‚ä½ æ˜¯ä¸€ä½ç¹é«”ä¸­æ–‡çš„æ‰¹åˆ¤æ€§åˆ†æå°ˆå®¶ï¼Œå°ˆé–€è©•ä¼°æ•…äº‹çš„è³ªé‡å’Œé€£è²«æ€§ã€‚
        åˆ†æé€™å€‹åŸºæ–¼å¤¢å¢ƒç‰‡æ®µå‰µä½œçš„æ•…äº‹ï¼Œæ‰¾å‡ºå®ƒçš„å„ªé»å’Œä¸è¶³ä¹‹è™•ï¼Œä¸¦æä¾›å…·é«”çš„æ”¹é€²å»ºè­°ã€‚è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ã€‚"""

        system_writer = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ã€‚ä½ æ˜¯ä¸€ä½ç¹é«”ä¸­æ–‡å°ˆç²¾æ–¼å¤¢å¢ƒæ•…äº‹å‰µä½œçš„ä½œå®¶ï¼Œèƒ½å°‡é›¶æ•£çš„å¤¢å¢ƒå…ƒç´ ç·¨ç¹”æˆå¼•äººå…¥å‹çš„æ•˜äº‹ã€‚
        ä½ çš„æ•…äº‹æ‡‰è©²èåˆæ‰€æœ‰æä¾›çš„å¤¢å¢ƒå…ƒç´ ï¼Œæ•æ‰å¤¢å¢ƒç‰¹æœ‰çš„è¶…ç¾å¯¦æ€§ï¼Œä¸¦ç¬¦åˆæŒ‡å®šçš„æƒ…ç·’æ°›åœã€‚è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ã€‚"""

        # åˆ†æå¤¢å¢ƒç‰‡æ®µä¸¦è¦åŠƒæ•…äº‹å¤§ç¶±
        cot_prompt = f"""
        ä½¿ç”¨è€…æä¾›äº†ä»¥ä¸‹å¤¢å¢ƒç‰‡æ®µï¼š{prompt}

        è«‹æ€è€ƒå¦‚ä½•å°‡é€™äº›å…ƒç´ çµ„ç¹”æˆæ•…äº‹ï¼š
        1. è­˜åˆ¥æ‰€æœ‰é—œéµå…ƒç´ èˆ‡å¯èƒ½çš„è±¡å¾µæ„ç¾©
        2. è€ƒæ…®å¤šç¨®å¯èƒ½çš„æ•…äº‹æ¶æ§‹ï¼ˆè‡³å°‘ä¸‰ç¨®ï¼‰
        3. è©•ä¼°æ¯å€‹æ¶æ§‹çš„å„ªç¼ºé»
        4. é¸æ“‡æœ€ä½³æ•…äº‹å¤§ç¶±
        5. ä½¿ç”¨15-30å­—æ•¸ç¸½çµæ•…äº‹å¤§ç¶±
        6. è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†
        """

        story_outline = ollama_generate(system_planner, cot_prompt, "qwen2.5:14b")
        if not story_outline:
            return "ç„¡æ³•ç”Ÿæˆæ•…äº‹å¤§ç¶±", "ç„¡æ³•æä¾›åé¥‹", "ç„¡æ³•ç”Ÿæˆæœ€çµ‚æ•…äº‹"

        # æ•…äº‹å‰µä½œ
        story_prompt = f"""
        åŸºæ–¼ä»¥ä¸‹æ€è€ƒéç¨‹å’Œå¤§ç¶±ï¼š

        {story_outline}

        è«‹å‰µä½œä¸€å€‹å®Œæ•´çš„å¤¢å¢ƒæ•…äº‹ï¼Œèåˆæ‰€æœ‰æä¾›çš„å¤¢å¢ƒå…ƒç´ ï¼šã€Œ{prompt}ã€ã€‚
        æ•…äº‹æ‡‰å±•ç¾å¤¢å¢ƒçš„è¶…ç¾å¯¦æ€§å’Œæµå‹•æ„Ÿï¼Œé•·åº¦50-100å­—ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
        """

        initial_story = ollama_generate(system_writer, story_prompt, "qwen2.5:14b")
        if not initial_story:
            initial_story = "ç„¡æ³•ç”Ÿæˆåˆå§‹æ•…äº‹"

        # è©•ä¼°æ•…äº‹ä¸¦æå‡ºæ”¹é€²å»ºè­°
        reflection_prompt = f"""
        ä»¥ä¸‹æ˜¯åŸºæ–¼ç”¨æˆ¶å¤¢å¢ƒç‰‡æ®µã€Œ{prompt}ã€å‰µä½œçš„åˆæ­¥æ•…äº‹ï¼š

        {initial_story}

        è«‹è©•ä¼°é€™å€‹æ•…äº‹ä¸¦æå‡ºå…·é«”æ”¹é€²å»ºè­°ã€‚é—œæ³¨ï¼š
        1. æ˜¯å¦æ‰€æœ‰å¤¢å¢ƒå…ƒç´ éƒ½å¾—åˆ°äº†æ°ç•¶èåˆï¼Ÿ
        2. æ•…äº‹çš„è¶…ç¾å¯¦æ€§å’Œå¤¢å¢ƒæ„Ÿå¦‚ä½•ï¼Ÿ
        3. å“ªäº›éƒ¨åˆ†å¯ä»¥å¢å¼·ä»¥ä½¿æ•…äº‹æ›´åŠ å¼•äººå…¥å‹ï¼Ÿ
        4. æ•…äº‹çµæ§‹å’Œé€£è²«æ€§å¦‚ä½•ï¼Ÿ
        5. çµ¦å‡º10-25å­—æ•¸çš„ç¸½çµæ€§è©•åƒ¹å’Œå»ºè­°ã€‚
        6. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
        """
        
        story_feedback = ollama_generate(system_reflector, reflection_prompt, "qwen2.5:14b")
        if not story_feedback:
            story_feedback = "ç„¡æ³•ç”Ÿæˆæ•…äº‹åé¥‹"

        # æ ¹æ“šåæ€å„ªåŒ–æ•…äº‹
        final_prompt = f"""
        è«‹æ ¹æ“šä»¥ä¸‹åé¥‹æ„è¦‹æ”¹é€²æ•…äº‹ï¼š

        {story_feedback}

        åŸå§‹æ•…äº‹ï¼š
        {initial_story}

        è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å‰µä½œæ›´åŠ å®Œå–„çš„æœ€çµ‚ç‰ˆæœ¬ï¼Œç¢ºä¿èåˆæ‰€æœ‰å¤¢å¢ƒå…ƒç´ ä¸¦å¢å¼·å…¶è¶…ç¾å¯¦æ€§ï¼Œå…¨éƒ¨éƒ½è¦ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚
        """
        
        final_story = ollama_generate(system_writer, final_prompt, "qwen2.5:14b")
        if not final_story:
            final_story = initial_story if initial_story != "ç„¡æ³•ç”Ÿæˆåˆå§‹æ•…äº‹" else "ç„¡æ³•ç”Ÿæˆæœ€çµ‚æ•…äº‹"

        return initial_story, story_feedback, final_story
    
    except Exception as e:
        logger.error(f"å¤¢å¢ƒç·¨ç¹”éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return "æ•…äº‹ç”Ÿæˆå¤±æ•—", "åé¥‹ç”Ÿæˆå¤±æ•—", "æœ€çµ‚æ•…äº‹ç”Ÿæˆå¤±æ•—"

def translate_to_english(text):
    """ä½¿ç”¨Ollamaçš„qwenæ¨¡å‹å°‡æ–‡æœ¬ç¿»è­¯æˆè‹±æ–‡"""
    try:
        if not text or text.strip() == "":
            return text

        # ç°¡å–®æª¢æŸ¥æ˜¯å¦å·²ç¶“æ˜¯è‹±æ–‡
        chinese_chars = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        if chinese_chars < len(text) * 0.3:  # å¦‚æœä¸­æ–‡å­—ç¬¦å°‘æ–¼30%ï¼Œå¯èƒ½å·²ç¶“æ˜¯è‹±æ–‡
            return text

        system_prompt = "ä½ æ˜¯ä¸€ä½ç¿»è­¯å°ˆå®¶ã€‚è«‹å°‡ç”¨æˆ¶è¼¸å…¥çš„ä»»ä½•èªè¨€ç¿»è­¯æˆè‹±æ–‡ï¼Œåªè¿”å›ç¿»è­¯çµæœï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡‹ã€‚è«‹å°‡æè¿°è½‰æ›ç‚ºé©åˆåœ–åƒç”Ÿæˆçš„ç°¡æ½”è‹±æ–‡æç¤ºè©ã€‚"
        user_prompt = f"å°‡ä»¥ä¸‹å¤¢å¢ƒæ•…äº‹ç¿»è­¯æˆç°¡æ½”çš„è‹±æ–‡åœ–åƒæè¿°: {text}"
        
        translation = ollama_generate(system_prompt, user_prompt, "qwen2.5:14b")
        
        # å¦‚æœç¿»è­¯å¤±æ•—æˆ–å¤ªé•·ï¼Œä½¿ç”¨ç°¡åŒ–ç‰ˆæœ¬
        if not translation or len(translation) > 500:
            return "dreamlike floating scene, blue ocean, golden sunlight, surreal clouds, ethereal atmosphere"
        
        return translation
    
    except Exception as e:
        logger.error(f"ç¿»è­¯éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return "dreamlike scene, floating, ocean, sunlight, surreal atmosphere"

def analyze_dream(image_path, video_path, text):
    """ä½¿ç”¨Ollamaåˆ†æå¤¢å¢ƒçš„å¿ƒç†æ„ç¾©"""
    try:
        system_prompt = """è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ã€‚ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å¤¢å¢ƒèˆ‡å¿ƒç†åˆ†æå°ˆå®¶ï¼Œæ“…é•·è§£è®€å¤¢å¢ƒçš„è±¡å¾µæ„ç¾©å’Œæ½›åœ¨çš„å¿ƒç†è¨Šæ¯ã€‚
        è«‹æ ¹æ“šä½¿ç”¨è€…æè¿°çš„å¤¢å¢ƒæä¾›æ·±å…¥çš„å¿ƒç†åˆ†æå’Œå»ºè­°ã€‚è«‹ç”¨å°ç£ç¿’æ…£çš„ä¸­æ–‡å›è¦†ï¼Œé¿å…ä½¿ç”¨éå¤šå¿ƒç†å­¸å°ˆæ¥­è¡“èªï¼Œç¢ºä¿å›ç­”é€šä¿—æ˜“æ‡‚ã€‚"""
        
        user_prompt = f"""
        ä»¥ä¸‹æ˜¯ä½¿ç”¨è€…æè¿°çš„å¤¢å¢ƒï¼š
        
        å¤¢å¢ƒæè¿°: {text}
        
        è«‹åˆ†æé€™å€‹å¤¢å¢ƒå¯èƒ½æ­ç¤ºçš„å¿ƒç†ç‹€æ…‹ã€æ½›æ„è­˜é¡˜æœ›æˆ–ææ‡¼ï¼Œä»¥åŠå¯èƒ½çš„è±¡å¾µæ„ç¾©ã€‚æä¾›å¿ƒç†å­¸è§€é»çš„è§£è®€ï¼Œ
        ä»¥åŠå°ä½¿ç”¨è€…ç•¶å‰ç”Ÿæ´»ç‹€æ…‹çš„å¯èƒ½å•Ÿç¤ºå’Œå»ºè­°ã€‚åˆ†æé•·åº¦æ§åˆ¶åœ¨150-200å­—å·¦å³ã€‚è«‹ä½¿ç”¨æº«å’Œã€æ”¯æŒæ€§çš„èªèª¿ã€‚
        """
        
        analysis = ollama_generate(system_prompt, user_prompt, "qwen2.5:14b")
        
        return analysis if analysis else "æš«æ™‚ç„¡æ³•é€²è¡Œå¿ƒç†åˆ†æï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    except Exception as e:
        logger.error(f"å¤¢å¢ƒåˆ†æéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return "å¿ƒç†åˆ†æåŠŸèƒ½æš«æ™‚ä¸å¯ç”¨ï¼Œä½†æ‚¨çš„å¤¢å¢ƒæè¿°å¾ˆæœ‰è¶£ï¼Œå»ºè­°æ‚¨è¨˜éŒ„ä¸‹ä¾†ä»¥ä¾¿æ—¥å¾Œå›é¡§ã€‚"

def ollama_generate(system_prompt, user_prompt, model="qwen2.5:14b"):
    """ä½¿ç”¨Ollama APIç”Ÿæˆæ–‡æœ¬"""
    try:
        data = {
            "model": model,
            "prompt": user_prompt,
            "system": system_prompt,
            "stream": False,
            "options": {
                "temperature": 0.7,
                "top_p": 0.9,
                "num_predict": 500,
                "stop": ["Human:", "Assistant:", "ç”¨æˆ¶:", "åŠ©æ‰‹:"]
            }
        }
        
        logger.info(f"ç™¼é€Ollamaè«‹æ±‚ï¼Œæ¨¡å‹: {model}")
        response = requests.post(OLLAMA_API, json=data, timeout=120)
        
        if response.status_code == 200:
            result = response.json()
            generated_text = result.get("response", "").strip()
            if generated_text:
                logger.info(f"OllamaæˆåŠŸç”Ÿæˆæ–‡æœ¬ï¼Œé•·åº¦: {len(generated_text)}")
                return generated_text
            else:
                logger.error("Ollamaè¿”å›ç©ºæ–‡æœ¬")
                return ""
        else:
            logger.error(f"Ollama APIéŒ¯èª¤: {response.status_code}, {response.text}")
            return ""
    except requests.exceptions.Timeout:
        logger.error("Ollamaè«‹æ±‚è¶…æ™‚")
        return ""
    except requests.exceptions.ConnectionError:
        logger.error("ç„¡æ³•é€£æ¥åˆ°Ollamaæœå‹™")
        return ""
    except Exception as e:
        logger.error(f"Ollamaè«‹æ±‚éŒ¯èª¤: {str(e)}")
        return ""

def create_default_image(filename, prompt_text=""):
    """å‰µå»ºé è¨­åœ–åƒ"""
    try:
        output_dir = os.path.join(static_dir, 'images')
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, filename)
        
        # å‰µå»ºæ¼¸è®ŠèƒŒæ™¯åœ–åƒ
        img = Image.new('RGB', (512, 512), color=(70, 130, 180))
        
        # å‰µå»ºæ¼¸è®Šæ•ˆæœ
        for y in range(512):
            for x in range(512):
                # å¾ä¸­å¿ƒå‘å¤–çš„æ¼¸è®Š
                center_x, center_y = 256, 256
                distance = ((x - center_x) ** 2 + (y - center_y) ** 2) ** 0.5
                max_distance = 256 * 1.414  # å°è§’ç·šé•·åº¦
                
                # æ ¹æ“šè·é›¢èª¿æ•´é¡è‰²
                factor = min(distance / max_distance, 1.0)
                r = int(70 + (120 - 70) * factor)
                g = int(130 + (80 - 130) * factor)
                b = int(180 + (200 - 180) * factor)
                
                img.putpixel((x, y), (r, g, b))
        
        # æ·»åŠ æ–‡å­—
        try:
            draw = ImageDraw.Draw(img)
            
            # å˜—è©¦è¼‰å…¥å­—é«”
            font_size = 32
            font = None
            
            # å˜—è©¦è¼‰å…¥å­—é«”
            font_paths = [
                # macOS
                "/System/Library/Fonts/Arial.ttf",
                "/System/Library/Fonts/Helvetica.ttc",
                "/System/Library/Fonts/PingFang.ttc",
                # Windows
                "C:/Windows/Fonts/arial.ttf",
                "C:/Windows/Fonts/calibri.ttf",
                # Linux
                "/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf",
                "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
            ]
            
            for font_path in font_paths:
                try:
                    if os.path.exists(font_path):
                        font = ImageFont.truetype(font_path, font_size)
                        break
                except:
                    continue
            
            if not font:
                try:
                    font = ImageFont.load_default()
                except:
                    font = None
            
            if font:
                # ä¸»æ¨™é¡Œ
                title_text = "å¤¢å¢ƒè¦–è¦ºåŒ–"
                bbox = draw.textbbox((0, 0), title_text, font=font)
                text_width = bbox[2] - bbox[0]
                text_height = bbox[3] - bbox[1]
                x = (512 - text_width) // 2
                y = 200
                
                # æ·»åŠ æ–‡å­—é™°å½±
                draw.text((x+2, y+2), title_text, fill=(0, 0, 0, 128), font=font)
                draw.text((x, y), title_text, fill=(255, 255, 255), font=font)
                
                # åº•éƒ¨æç¤º
                info_text = "AI ç”Ÿæˆä¸­..."
                try:
                    info_font = ImageFont.truetype(font_paths[0], 18) if font_paths else font
                except:
                    info_font = font
                
                if info_font:
                    bbox = draw.textbbox((0, 0), info_text, font=info_font)
                    text_width = bbox[2] - bbox[0]
                    x = (512 - text_width) // 2
                    y = 350
                    
                    draw.text((x+1, y+1), info_text, fill=(0, 0, 0, 64), font=info_font)
                    draw.text((x, y), info_text, fill=(200, 200, 200), font=info_font)
        
        except Exception as text_error:
            logger.info(f"æ·»åŠ æ–‡å­—æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(text_error)}")
        
        img.save(output_path, 'PNG')
        logger.info(f"å‰µå»ºé è¨­åœ–åƒ: {filename}")
        return os.path.join('images', filename)
    
    except Exception as e:
        logger.error(f"å‰µå»ºé è¨­åœ–åƒå¤±æ•—: {str(e)}")
        return "images/default_dream.png"

def save_dream_result(data):
    """ä¿å­˜å¤¢å¢ƒåˆ†æçµæœä»¥ä¾¿åˆ†äº«"""
    try:
        # å‰µå»ºå”¯ä¸€ID
        share_id = str(uuid.uuid4())
        
        # å‰µå»ºå­˜å„²ç›®éŒ„
        share_dir = os.path.join(static_dir, 'shares')
        os.makedirs(share_dir, exist_ok=True)
        
        # åŒ…å«æ™‚é–“æˆ³ä»¥æ–¹ä¾¿æ’åº
        timestamp = int(time.time())
        
        # æ§‹å»ºä¿å­˜æ•¸æ“š
        share_data = {
            'id': share_id,
            'timestamp': timestamp,
            'created_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'initialStory': data.get('initialStory', ''),
            'storyFeedback': data.get('storyFeedback', ''),
            'finalStory': data.get('finalStory', ''),
            'translation': data.get('translation', ''),
            'imagePath': data.get('imagePath', ''),
            'videoPath': data.get('videoPath', ''),
            'psychologyAnalysis': data.get('psychologyAnalysis', '')
        }
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        share_file = os.path.join(share_dir, f"{share_id}.json")
        with open(share_file, 'w', encoding='utf-8') as f:
            json.dump(share_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ä¿å­˜åˆ†äº«æ•¸æ“šæˆåŠŸ: {share_id}")
        return share_id
    except Exception as e:
        logger.error(f"ä¿å­˜åˆ†äº«æ•¸æ“šæ™‚å‡ºéŒ¯: {str(e)}")
        return None

# ==================== è¨˜æ†¶é«”ç®¡ç† ====================

def clear_model_memory():
    """æ¸…ç†æ¨¡å‹è¨˜æ†¶é«”"""
    global image_pipe, video_pipe, models_loaded
    
    import gc
    
    if image_pipe is not None:
        del image_pipe
        image_pipe = None
    
    if video_pipe is not None:
        del video_pipe  
        video_pipe = None
    
    models_loaded = False
    gc.collect()
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    elif torch.backends.mps.is_available():
        torch.mps.empty_cache()
    
    logger.info("æ¨¡å‹è¨˜æ†¶é«”å·²æ¸…ç†")

# ==================== è·¯ç”±å®šç¾© ====================

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/api/status')
def api_status():
    """æª¢æŸ¥æœå‹™ç‹€æ…‹"""
    ollama_status, fooocus_status, local_models_status = check_local_services()
    
    return jsonify({
        'ollama': ollama_status,
        'fooocus': fooocus_status,  # ä¿æŒå…¼å®¹æ€§
        'framepack': local_models_status,  # ç¾åœ¨æŒ‡å‘æœ¬åœ°æ¨¡å‹
        'local_models': local_models_status,
        'timestamp': int(time.time())
    })

@app.route('/api/analyze', methods=['POST'])
def analyze():
    data = request.json
    dream_text = data.get('dream', '')
    
    if not dream_text or len(dream_text.strip()) < 10:
        return jsonify({'error': 'è«‹è¼¸å…¥è‡³å°‘10å€‹å­—çš„å¤¢å¢ƒæè¿°'}), 400
    
    if len(dream_text.strip()) > 2000:
        return jsonify({'error': 'å¤¢å¢ƒæè¿°éé•·ï¼Œè«‹æ§åˆ¶åœ¨2000å­—ä»¥å…§'}), 400
    
    try:
        logger.info(f"é–‹å§‹è™•ç†å¤¢å¢ƒåˆ†æè«‹æ±‚ï¼Œè¼¸å…¥é•·åº¦: {len(dream_text)}")
        
        # æª¢æŸ¥æœå‹™ç‹€æ…‹
        ollama_status, _, local_models_status = check_local_services()
        if not ollama_status:
            return jsonify({'error': 'Ollamaæœå‹™ä¸å¯ç”¨ï¼Œè«‹ç¢ºèªæœå‹™æ˜¯å¦æ­£å¸¸é‹è¡Œåœ¨ localhost:11434'}), 503
        
        # æ­¥é©Ÿ1: ä½¿ç”¨dream_weaverè™•ç†å¤¢å¢ƒæ•…äº‹
        logger.info("æ­¥é©Ÿ1: é–‹å§‹å¤¢å¢ƒæ•…äº‹ç”Ÿæˆ")
        initial_story, story_feedback, final_story = dream_weaver(dream_text)
        
        # æ­¥é©Ÿ2: ç¿»è­¯æ•…äº‹ä»¥ä¾¿æ›´å¥½åœ°ç”Ÿæˆåœ–åƒï¼ˆä¿ç•™å‚™ç”¨ï¼‰
        logger.info("æ­¥é©Ÿ2: é–‹å§‹ç¿»è­¯æ•…äº‹")
        translation = translate_to_english(final_story)
        
        # æ­¥é©Ÿ3: ä½¿ç”¨æœ¬åœ°å¿«é€Ÿæ¨¡å‹ç”Ÿæˆåœ–åƒ
        logger.info("æ­¥é©Ÿ3: é–‹å§‹ç”Ÿæˆåœ–åƒ")
        if local_models_status:
            image_path = generate_image_fast_local(final_story)
        else:
            logger.warning("æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ï¼Œä½¿ç”¨é è¨­åœ–åƒ")
            timestamp = int(time.time())
            image_path = create_default_image(f"default_{timestamp}.png", final_story)
        
        # æ­¥é©Ÿ4: ä½¿ç”¨æœ¬åœ°å¿«é€Ÿæ¨¡å‹ç”Ÿæˆè¦–é »
        logger.info("æ­¥é©Ÿ4: é–‹å§‹ç”Ÿæˆè¦–é »")
        video_path = None
        if local_models_status and image_path:
            video_path = generate_video_fast_local(image_path, final_story)
            if video_path:
                logger.info("âœ… è¦–é »ç”ŸæˆæˆåŠŸ")
            else:
                logger.warning("âš ï¸ è¦–é »ç”Ÿæˆå¤±æ•—ï¼Œä½†ä¸å½±éŸ¿å…¶ä»–åŠŸèƒ½")
        else:
            logger.warning("æœ¬åœ°æ¨¡å‹ä¸å¯ç”¨ï¼Œè·³éè¦–é »ç”Ÿæˆ")
        
        # æ­¥é©Ÿ5: å¿ƒç†åˆ†æ
        logger.info("æ­¥é©Ÿ5: é–‹å§‹å¿ƒç†åˆ†æ")
        psychology_analysis = analyze_dream(image_path, video_path, dream_text)
        
        # æº–å‚™éŸ¿æ‡‰
        response = {
            'initialStory': initial_story,
            'storyFeedback': story_feedback,
            'finalStory': final_story,
            'translation': translation,
            'imagePath': '/static/' + image_path if image_path else None,
            'videoPath': '/static/' + video_path if video_path else None,
            'psychologyAnalysis': psychology_analysis,
            'apiStatus': {
                'ollama': ollama_status,
                'fooocus': True,  # ä¿æŒå…¼å®¹æ€§
                'framepack': local_models_status,
                'local_models': local_models_status
            },
            'processingInfo': {
                'timestamp': int(time.time()),
                'inputLength': len(dream_text),
                'storyLength': len(final_story) if final_story else 0,
                'useLocalModels': True
            }
        }
        
        logger.info("å¤¢å¢ƒåˆ†æå®Œæˆï¼Œæº–å‚™è¿”å›çµæœ")
        return jsonify(response)
    
    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        logger.error(f"è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}\n{error_details}")
        return jsonify({
            'error': f'è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}',
            'details': error_details if app.debug else None
        }), 500

@app.route('/api/share', methods=['POST'])
def share_result():
    """å‰µå»ºå¯åˆ†äº«çš„å¤¢å¢ƒåˆ†æçµæœ"""
    data = request.json
    
    if not data or 'finalStory' not in data:
        return jsonify({'error': 'ç¼ºå°‘å¿…è¦çš„å¤¢å¢ƒåˆ†ææ•¸æ“š'}), 400
    
    try:
        # ä¿å­˜åˆ†äº«æ•¸æ“š
        share_id = save_dream_result(data)
        
        if not share_id:
            return jsonify({'error': 'å‰µå»ºåˆ†äº«å¤±æ•—'}), 500
        
        # å‰µå»ºåˆ†äº«URL
        share_url = url_for('view_shared', share_id=share_id, _external=True)
        
        return jsonify({
            'shareId': share_id, 
            'shareUrl': share_url,
            'timestamp': int(time.time())
        })
    
    except Exception as e:
        logger.error(f'è™•ç†åˆ†äº«è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}')
        return jsonify({'error': f'è™•ç†åˆ†äº«è«‹æ±‚æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}'}), 500

@app.route('/share/<share_id>')
def view_shared(share_id):
    """æŸ¥çœ‹åˆ†äº«çš„å¤¢å¢ƒåˆ†æçµæœ"""
    try:
        # æª¢æŸ¥åˆ†äº«IDæ ¼å¼
        if not share_id or not all(c.isalnum() or c == '-' for c in share_id):
            return jsonify({'error': 'ç„¡æ•ˆçš„åˆ†äº«ID'}), 400
        
        # è®€å–åˆ†äº«æ•¸æ“š
        share_file = os.path.join(static_dir, 'shares', f"{share_id}.json")
        
        if not os.path.exists(share_file):
            return jsonify({'error': 'æ‰¾ä¸åˆ°è©²åˆ†äº«å…§å®¹'}), 404
        
        with open(share_file, 'r', encoding='utf-8') as f:
            share_data = json.load(f)
        
        # æ¸²æŸ“åˆ†äº«é é¢
        try:
            return render_template('shared.html', data=share_data)
        except:
            return render_template('index.html', shared_data=share_data)
    
    except Exception as e:
        logger.error(f'è¼‰å…¥åˆ†äº«å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}')
        return jsonify({'error': f'è¼‰å…¥åˆ†äº«å…§å®¹æ™‚ç™¼ç”ŸéŒ¯èª¤'}), 500

@app.errorhandler(404)
def not_found_error(error):
    return jsonify({'error': 'é é¢ä¸å­˜åœ¨'}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({'error': 'æœå‹™å™¨å…§éƒ¨éŒ¯èª¤'}), 500

# ==================== ä¸»ç¨‹å¼å…¥å£ ====================

if __name__ == '__main__':
    try:
        # åœ¨ç¨‹å¼çµæŸæ™‚æ¸…ç†è¨˜æ†¶é«”
        import atexit
        atexit.register(clear_model_memory)
        
        # ç¢ºä¿å¿…è¦çš„ç›®éŒ„å­˜åœ¨
        directories = [
            os.path.join(static_dir, 'images'),
            os.path.join(static_dir, 'videos'),
            os.path.join(static_dir, 'shares')
        ]
        
        for directory in directories:
            os.makedirs(directory, exist_ok=True)
            logger.info(f"ç¢ºä¿ç›®éŒ„å­˜åœ¨: {directory}")
        
        # ç¢ºä¿æœ‰é è¨­åœ–åƒ
        default_image_path = os.path.join(static_dir, 'images', 'default_dream.png')
        if not os.path.exists(default_image_path):
            try:
                logger.info("å‰µå»ºé è¨­åœ–åƒ...")
                create_default_image('default_dream.png', 'å¤¢å¢ƒç·¨ç¹”è€…ç³»çµ±')
                logger.info("é è¨­åœ–åƒå‰µå»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"ç„¡æ³•å‰µå»ºé è¨­åœ–åƒ: {str(e)}")
        
        # æª¢æŸ¥æœå‹™ç‹€æ…‹
        logger.info("æª¢æŸ¥æœå‹™ç‹€æ…‹...")
        ollama_status, _, local_models_status = check_local_services()
        
        # è¼¸å‡ºç‹€æ…‹å ±å‘Š
        print("=" * 80)
        print("å¤¢å¢ƒç·¨ç¹”è€…ç³»çµ± - æœ¬åœ°å¿«é€Ÿç”Ÿæˆç‰ˆæœ¬ å•Ÿå‹•ç‹€æ…‹å ±å‘Š")
        print("=" * 80)
        print(f"Ollama API (localhost:11434): {'âœ… æ­£å¸¸' if ollama_status else 'âŒ ç•°å¸¸'}")
        print(f"æœ¬åœ°åœ–åƒç”Ÿæˆæ¨¡å‹: {'âœ… å¯ç”¨' if local_models_status else 'âŒ ä¸å¯ç”¨'}")
        print(f"æœ¬åœ°è¦–é »ç”Ÿæˆæ¨¡å‹: {'âœ… å¯ç”¨' if local_models_status else 'âŒ ä¸å¯ç”¨'}")
        print(f"éœæ…‹æª”æ¡ˆç›®éŒ„: {static_dir}")
        
        # æª¢æŸ¥ PyTorch å’Œè¨­å‚™æ”¯æŒ
        if torch.backends.mps.is_available():
            print("âœ… å·²å•Ÿç”¨ Metal Performance Shaders (MPS) åŠ é€Ÿ")
            device_info = "MPS (Apple Silicon å„ªåŒ–)"
        elif torch.cuda.is_available():
            print("âœ… å·²å•Ÿç”¨ CUDA åŠ é€Ÿ")
            device_info = "CUDA"
        else:
            print("âš ï¸  ä½¿ç”¨ CPU æ¨¡å¼ï¼Œé€Ÿåº¦å¯èƒ½è¼ƒæ…¢")
            device_info = "CPU"
        
        print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
        print(f"ä½¿ç”¨è¨­å‚™: {device_info}")
        print("=" * 80)
        
        # è©³ç´°çš„ç‹€æ…‹èªªæ˜å’Œå»ºè­°
        if not ollama_status:
            print("âŒ è­¦å‘Š: Ollama API ç„¡æ³•é€£æ¥")
            print("   è«‹ç¢ºèª Ollama æœå‹™æ˜¯å¦é‹è¡Œåœ¨ localhost:11434")
            print("   å•Ÿå‹•å‘½ä»¤: ollama serve")
            print()
        
        if not local_models_status:
            print("âŒ è­¦å‘Š: æœ¬åœ°ç”Ÿæˆæ¨¡å‹ä¸å¯ç”¨")
            print("   é¦–æ¬¡é‹è¡Œæ™‚æœƒè‡ªå‹•ä¸‹è¼‰æ¨¡å‹ï¼ˆç´„ 4-6GBï¼‰")
            print("   è«‹ç¢ºä¿ç¶²è·¯é€£æ¥æ­£å¸¸ä¸”æœ‰è¶³å¤ çš„å„²å­˜ç©ºé–“")
            print("   æ¨¡å‹ä¸‹è¼‰å®Œæˆå¾Œå³å¯é›¢ç·šä½¿ç”¨")
            print()
        
        # ç³»çµ±åŠŸèƒ½èªªæ˜
        print("ğŸ”§ ç³»çµ±åŠŸèƒ½ç‹€æ…‹:")
        print(f"   â€¢ æ•…äº‹ç”Ÿæˆ: {'âœ… å¯ç”¨ (Ollama)' if ollama_status else 'âŒ ä¸å¯ç”¨'}")
        print(f"   â€¢ æ–‡æœ¬ç¿»è­¯: {'âœ… å¯ç”¨ (Ollama)' if ollama_status else 'âŒ ä¸å¯ç”¨'}")
        print(f"   â€¢ åœ–åƒç”Ÿæˆ: {'âœ… å¯ç”¨ (Stable Diffusion v1.5)' if local_models_status else 'âš ï¸  é è¨­åœ–åƒ'}")
        print(f"   â€¢ è¦–é »ç”Ÿæˆ: {'âœ… å¯ç”¨ (Stable Video Diffusion)' if local_models_status else 'âŒ ä¸å¯ç”¨'}")
        print(f"   â€¢ å¿ƒç†åˆ†æ: {'âœ… å¯ç”¨ (Ollama)' if ollama_status else 'âŒ ä¸å¯ç”¨'}")
        print(f"   â€¢ æ•…äº‹è½‰æç¤ºè©: {'âœ… å¯ç”¨ (æ™ºèƒ½è½‰æ›)' if ollama_status else 'âš ï¸  é—œéµè©æå–'}")
        print()
        
        # ç‰¹æ®Šèªªæ˜
        print("ğŸš€ æœ¬åœ°å¿«é€Ÿç”Ÿæˆç‰¹æ€§:")
        if local_models_status:
            print("   â€¢ åœ–åƒç”Ÿæˆ: 10-25 ç§’ï¼ˆ512x512ï¼‰")
            print("   â€¢ è¦–é »ç”Ÿæˆ: 30 ç§’-2 åˆ†é˜ï¼ˆ0.6 ç§’@8fpsï¼‰")
            print("   â€¢ æ™ºèƒ½æ•…äº‹è½‰åœ–åƒæç¤ºè©")
            print("   â€¢ å®Œå…¨é›¢ç·šé‹è¡Œï¼ˆé¦–æ¬¡ä¸‹è¼‰å¾Œï¼‰")
            print("   â€¢ è‡ªå‹•è¨­å‚™å„ªåŒ–ï¼ˆMPS/CUDA/CPUï¼‰")
        print("   â€¢ æ‰€æœ‰åŠŸèƒ½é€šéçµ±ä¸€ç•Œé¢ä½¿ç”¨")
        print("   â€¢ å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„")
        print("   â€¢ è‡ªå‹•è¨˜æ†¶é«”ç®¡ç†")
        print()
        
        # æ€§èƒ½é æœŸ
        print("âš¡ æ€§èƒ½é æœŸ:")
        if device_info == "MPS (Apple Silicon å„ªåŒ–)":
            print("   â€¢ M1/M2/M3/M4 å„ªåŒ–ï¼Œé€Ÿåº¦è¼ƒå¿«")
            print("   â€¢ è¨˜æ†¶é«”ä½¿ç”¨: 4-8GB")
        elif device_info == "CUDA":
            print("   â€¢ GPU åŠ é€Ÿï¼Œé€Ÿåº¦æœ€å¿«")
            print("   â€¢ è¨˜æ†¶é«”ä½¿ç”¨: 4-6GB VRAM")
        else:
            print("   â€¢ CPU æ¨¡å¼ï¼Œé€Ÿåº¦è¼ƒæ…¢ä½†åŠŸèƒ½å®Œæ•´")
            print("   â€¢ è¨˜æ†¶é«”ä½¿ç”¨: 6-12GB RAM")
        print()
        
        # æœ€ä½é‹è¡Œè¦æ±‚
        if ollama_status:
            print("âœ… ç³»çµ±å¯ä»¥åŸºæœ¬é‹è¡Œï¼ˆè‡³å°‘éœ€è¦ Ollamaï¼‰")
        else:
            print("âŒ ç³»çµ±ç„¡æ³•æ­£å¸¸é‹è¡Œï¼Œéœ€è¦è‡³å°‘å®‰è£ Ollama")
        
        print("=" * 80)
        print("ç³»çµ±æº–å‚™å°±ç·’ï¼Œå•Ÿå‹• Flask æ‡‰ç”¨ç¨‹å¼...")
        print("è¨ªå•åœ°å€: http://localhost:5002")
        print("=" * 80)
        
        # å•Ÿå‹•Flaskæ‡‰ç”¨
        app.run(debug=True, host='0.0.0.0', port=5002, threaded=True)
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ¶ä¸­æ–·ï¼Œæ­£åœ¨é—œé–‰ç³»çµ±...")
        clear_model_memory()
    except Exception as e:
        logger.error(f"ç³»çµ±å•Ÿå‹•å¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        clear_model_memory()